{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9da748",
   "metadata": {},
   "source": [
    "**Abstract**: This program uses Regular Expression (`re`) and Natural Language Toolkit (`nltk`) to clean raw post data and collect some features of the data. It uses object-oriented programming (OOP) strategy and creates father class `Data_to_Clean` and derived class `Data_to_Analyze` including various methods to clean and analyze data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bedaa",
   "metadata": {},
   "source": [
    "### Import modules and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b952557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "# Module to load raw data(CSV file)\n",
    "import pandas as pd\n",
    "\n",
    "# Modules for NLP\n",
    "import re # Regular Expression\n",
    "import string\n",
    "from typing import List\n",
    "import nltk # Natural Language Toolkit\n",
    "from nltk.tokenize import word_tokenize # For text tokenization\n",
    "from nltk.corpus import stopwords,wordnet # For stopwords removal\n",
    "# For tokens part-of-speech tagging and lemmatization\n",
    "from nltk import pos_tag \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "my_nltk_path=\"Data\"\n",
    "nltk.data.path.append(my_nltk_path)\n",
    "import textstat # Evaluate text readability\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer # Evaluate text emotion\n",
    "# Transformers model to evaluate text emotion\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Modules to read/write external files,etc.\n",
    "import json\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "# Average function\n",
    "def ave(l):\n",
    "    return sum(l)/len(l)\n",
    "\n",
    "# MBTI type dictionary\n",
    "MBTI_types = [\n",
    "    \"ISTJ\", \"ISFJ\", \"INFJ\", \"INTJ\",\n",
    "    \"ISTP\", \"ISFP\", \"INFP\", \"INTP\",\n",
    "    \"ESTP\", \"ESFP\", \"ENFP\", \"ENTP\",\n",
    "    \"ESTJ\", \"ESFJ\", \"ENFJ\", \"ENTJ\"\n",
    "]\n",
    "\n",
    "# Data loading and spliting \n",
    "raw_data=pd.read_csv(\"Data\\\\mbti_1.csv\")\n",
    "for i in raw_data.index:\n",
    "    temp=raw_data.loc[i,\"posts\"]\n",
    "    temp=temp.split(\"|||\")\n",
    "    raw_data.loc[i,\"posts\"]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "378cfd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>INFP</td>\n",
       "      <td>['I think we do agree. I personally don't cons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>INFP</td>\n",
       "      <td>['That's normal, it happens also to me. If I a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>INFP</td>\n",
       "      <td>['45016 urh sorry uh. couldn't resist., all of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>INFP</td>\n",
       "      <td>['Personally, I was thinking this would be mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>INFP</td>\n",
       "      <td>['Basically, my main questions are : What do y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8649</th>\n",
       "      <td>INFP</td>\n",
       "      <td>[https://www.youtube.com/watch?v=nPDusM-75FE  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653</th>\n",
       "      <td>INFP</td>\n",
       "      <td>['You should do whatever it is you want. If yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8660</th>\n",
       "      <td>INFP</td>\n",
       "      <td>['I love the vastness of the sky, because my p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>INFP</td>\n",
       "      <td>['I am very conflicted right now when it comes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>INFP</td>\n",
       "      <td>['It has been too long since I have been on pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1832 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts\n",
       "17    INFP  ['I think we do agree. I personally don't cons...\n",
       "19    INFP  ['That's normal, it happens also to me. If I a...\n",
       "23    INFP  ['45016 urh sorry uh. couldn't resist., all of...\n",
       "25    INFP  ['Personally, I was thinking this would be mor...\n",
       "28    INFP  ['Basically, my main questions are : What do y...\n",
       "...    ...                                                ...\n",
       "8649  INFP  [https://www.youtube.com/watch?v=nPDusM-75FE  ...\n",
       "8653  INFP  ['You should do whatever it is you want. If yo...\n",
       "8660  INFP  ['I love the vastness of the sky, because my p...\n",
       "8673  INFP  ['I am very conflicted right now when it comes...\n",
       "8674  INFP  ['It has been too long since I have been on pe...\n",
       "\n",
       "[1832 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[raw_data[\"type\"]==\"INFP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18c9b9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "INFP    1832\n",
       "INFJ    1470\n",
       "INTP    1304\n",
       "INTJ    1091\n",
       "ENTP     685\n",
       "ENFP     675\n",
       "ISTP     337\n",
       "ISFP     271\n",
       "ENTJ     231\n",
       "ISTJ     205\n",
       "ENFJ     190\n",
       "ISFJ     166\n",
       "ESTP      89\n",
       "ESFP      48\n",
       "ESFJ      42\n",
       "ESTJ      39\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6b340",
   "metadata": {},
   "source": [
    "### Create a class to clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0258b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_to_Clean:\n",
    "\n",
    "    # Load the contraction map in class\n",
    "    with open(file=\"contractions.json\",mode='r',encoding='utf-8') as f:\n",
    "        contractions_map=json.load(f)\n",
    "    def __init__(self,source=raw_data):\n",
    "        #self.data should be ALL THE POSTS, type:pd.Series\n",
    "        self.data=source\n",
    "        \n",
    "    # Remove URL\n",
    "    def remove_url(self):\n",
    "        def process_remove_url(post):\n",
    "            post_without_url=[]\n",
    "            for sentence in post:\n",
    "                # Use re to scan and substitute\n",
    "                post_without_url.append(\n",
    "                re.sub(\n",
    "                    pattern=r'http\\S+|www\\S+|https\\S+',\n",
    "                    repl='',\n",
    "                    string=sentence,\n",
    "                    flags=re.MULTILINE\n",
    "                    )\n",
    "                )\n",
    "            return post_without_url\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_remove_url)\n",
    "    \n",
    "    # Expand contractions\n",
    "    @staticmethod\n",
    "    def text_expand(original_string,contraction_mapping=contractions_map):\n",
    "        # Compile an re pattern\n",
    "        contractions_pattern = re.compile(\n",
    "            '({})'.format('|'.join(contraction_mapping.keys())),\n",
    "            flags=re.IGNORECASE|re.DOTALL\n",
    "            )\n",
    "        # Map original string to expanded string\n",
    "        def text_mapping(text_matched):\n",
    "            old_text=text_matched.group(0)\n",
    "            new_text=contraction_mapping.get(old_text.lower())\n",
    "            if not new_text:\n",
    "                new_text=contraction_mapping.get(old_text)\n",
    "                if not new_text:\n",
    "                    return old_text\n",
    "            return new_text\n",
    "        # Use re.sub() to scan and substitute\n",
    "        expanded_string=contractions_pattern.sub(\n",
    "            repl=lambda m:text_mapping(m),\n",
    "            string=original_string\n",
    "        )\n",
    "        return expanded_string\n",
    "    # Apply the function to dataset\n",
    "    def expand_contractions(self):\n",
    "        def process_expand_contractions(original_list):\n",
    "            for idx in range(len(original_list)):\n",
    "                original_list[idx]=Data_to_Clean.text_expand(original_list[idx])\n",
    "            return original_list\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(lambda x:process_expand_contractions(x))\n",
    "\n",
    "    # Convert to lower case\n",
    "    def tolower(self):\n",
    "        def process_tolower(post):\n",
    "            return [\n",
    "                sentence.lower() for sentence in post\n",
    "            ]\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_tolower)\n",
    "    \n",
    "    # Remove punctuations\n",
    "    def remove_punct(self):\n",
    "        def process_remove_punct(post):\n",
    "            post_without_punct=[]\n",
    "            for sentence in post:\n",
    "                post_without_punct.append(\n",
    "                    re.sub(\n",
    "                    pattern=r'[^a-zA-Z\\s]',\n",
    "                    repl=' ',\n",
    "                    string=sentence\n",
    "                    )\n",
    "                )\n",
    "            return post_without_punct\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_remove_punct)\n",
    "        \n",
    "    # Remove empty string and whitespace characters\n",
    "    def remove_whitespace(self):\n",
    "        def process_remove_whitespace(post):\n",
    "            return [\n",
    "                sentence for sentence in post if sentence.strip()\n",
    "            ]\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_remove_whitespace)\n",
    "\n",
    "    # Text tokenization\n",
    "    def totokens(self):\n",
    "        def process_totokens(post):\n",
    "            post_totokens=[]\n",
    "            for sentence in post:\n",
    "                tokens=word_tokenize(sentence)\n",
    "                post_totokens.append(tokens)\n",
    "            return post_totokens\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_totokens)\n",
    "    \n",
    "    # Remove stopwords in tokenized text\n",
    "    def remove_stopwords(self):\n",
    "        def process_remove_stopwords(post):\n",
    "            stop_words=set(stopwords.words(\"english\"))\n",
    "            filtered_post=[]\n",
    "            for sentence in post:\n",
    "                filtered_sentence=[]\n",
    "                for word in sentence:\n",
    "                    if word not in stop_words:\n",
    "                        filtered_sentence.append(word)\n",
    "                filtered_post.append(filtered_sentence)\n",
    "            return filtered_post\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_remove_stopwords)\n",
    "\n",
    "    # Lemmatization\n",
    "    def post_lemmatize(self):\n",
    "        def process_lemmatize(post):\n",
    "            # Convert format of part-of-speech tags\n",
    "            def get_wordnet_postag(old_postag):\n",
    "                if old_postag.startswith('J'):  \n",
    "                    return wordnet.ADJ \n",
    "                elif old_postag.startswith('V'):  \n",
    "                    return wordnet.VERB\n",
    "                elif old_postag.startswith('N'):  \n",
    "                    return wordnet.NOUN  \n",
    "                elif old_postag.startswith('R'):  \n",
    "                    return wordnet.ADV  \n",
    "                else:  \n",
    "                    return wordnet.NOUN\n",
    "            lemmatizer=WordNetLemmatizer()\n",
    "            lemmatized_post=[]\n",
    "            for tokens in post:\n",
    "                lemmatized_tokens=[]\n",
    "                # Part of speech tagging\n",
    "                tagged_tokens=pos_tag(tokens)\n",
    "                # Lemmatize tokens\n",
    "                for word,tag in tagged_tokens:\n",
    "                    lemmatized_tokens.append(lemmatizer.lemmatize(word,get_wordnet_postag(tag)))\n",
    "                lemmatized_post.append(lemmatized_tokens)\n",
    "            return lemmatized_post\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_lemmatize)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b717ca2d",
   "metadata": {},
   "source": [
    "### Create a derived class to analysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "977a9c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_to_Analyze(Data_to_Clean):\n",
    "    def __init__(self,type,source=raw_data):\n",
    "        # First initialize an object of father class(Data_to_Clean)\n",
    "        super().__init__(source)\n",
    "        # self.data is of type pd.DataFrame, now specific the MBTI type\n",
    "        self.data=self.data.loc[self.data[\"type\"]==type].reset_index(drop=True)\n",
    "        self.data_to_vec=None\n",
    "        # Store bacic identities of the text\n",
    "        self.basic_identities=pd.Series({\n",
    "\n",
    "            \"type\":type,\n",
    "            # Number of sentences in a post\n",
    "            \"sentence_quantity\":[],\n",
    "            \"ave_sentence_quantity\":None,\n",
    "            # Number of words in a post\n",
    "            \"word_count\":[],\n",
    "            \"ave_word_count\":None,\n",
    "            # Ratio of upper case characters in a post\n",
    "            \"upper_ratio\":[],\n",
    "            \"ave_upper_ratio\":None,\n",
    "            # Two indicators of text readability: Flesch Reading Ease and Gunning Fog Index \n",
    "            \"reading_ease\":[],\n",
    "            \"ave_reading_ease\":None,\n",
    "            \"GF_index\":[],\n",
    "            \"ave_GF_index\":None,\n",
    "            # Overall text emotion indicator\n",
    "            \"overall_vader_score\":None\n",
    "        })\n",
    "\n",
    "    # Design various methods to get identity data\n",
    "\n",
    "    def get_sentence_quantity(self):\n",
    "        for post in self.data[\"posts\"].values:\n",
    "            self.basic_identities[\"sentence_quantity\"].append(len(post))\n",
    "        self.basic_identities[\"ave_sentence_quantity\"]=ave(self.basic_identities[\"sentence_quantity\"])\n",
    "    \n",
    "    def get_word_count(self):\n",
    "        for post in self.data[\"posts\"].values:\n",
    "            ans=0\n",
    "            for sentence in post:\n",
    "                ans+=len(sentence.split(\" \"))\n",
    "            self.basic_identities[\"word_count\"].append(ans)\n",
    "        self.basic_identities[\"ave_word_count\"]=ave(self.basic_identities[\"word_count\"])\n",
    " \n",
    "    def get_upper_ratio(self):\n",
    "        for post in self.data[\"posts\"].values:\n",
    "            char_count=0;upper_count=0\n",
    "            for sentence in post:\n",
    "                for char in sentence:\n",
    "                    if char.isalpha():\n",
    "                        char_count+=1\n",
    "                        if char.isupper():\n",
    "                            upper_count+=1\n",
    "            if char_count!=0:\n",
    "                self.basic_identities[\"upper_ratio\"].append(upper_count/char_count)\n",
    "            else:\n",
    "                continue\n",
    "        self.basic_identities[\"ave_upper_ratio\"]=ave(self.basic_identities[\"upper_ratio\"])\n",
    "    \n",
    "    def get_readability(self):\n",
    "        reading_ease=[];GF_idx=[]\n",
    "        for post in self.data[\"posts\"].values:\n",
    "            concatenated_post=post[0]\n",
    "            for idx in range(1,len(post)):\n",
    "                concatenated_post+=post[idx]\n",
    "            reading_ease.append(\n",
    "                textstat.flesch_reading_ease(concatenated_post)\n",
    "            )\n",
    "            GF_idx.append(\n",
    "                textstat.gunning_fog(concatenated_post)\n",
    "            )\n",
    "        self.basic_identities[\"reading_ease\"]=reading_ease\n",
    "        self.basic_identities[\"ave_reading_ease\"]=ave(self.basic_identities[\"reading_ease\"])\n",
    "        self.basic_identities[\"GF_index\"]=GF_idx\n",
    "        self.basic_identities[\"ave_GF_index\"]=ave(self.basic_identities[\"GF_index\"])\n",
    "    @staticmethod\n",
    "    def concatenate_full_post(post):\n",
    "                filtered_post=[sentence for sentence in post if not sentence.isspace()]\n",
    "                return \"\".join(filtered_post)\n",
    "    def get_vader_score(self):\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        overall_vader_score={'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "        def addup_score_dict(new_dict,base_dict):\n",
    "            for key in base_dict.keys():\n",
    "                base_dict[key]+=new_dict[key]\n",
    "        def ave_score_dict(base_dict,n):\n",
    "            for key in base_dict.keys():\n",
    "                base_dict[key]/=n\n",
    "        def process_vader_score(post):\n",
    "            post_vader_score={'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "            for sentence in post:\n",
    "               addup_score_dict(analyzer.polarity_scores(sentence),base_dict=post_vader_score) \n",
    "            ave_score_dict(base_dict=post_vader_score,n=len(post))\n",
    "            addup_score_dict(new_dict=post_vader_score,base_dict=overall_vader_score)\n",
    "            return post_vader_score\n",
    "        self.data[\"vader_score\"]=self.data[\"posts\"].apply(process_vader_score)\n",
    "        ave_score_dict(overall_vader_score,len(self.data[\"posts\"]))\n",
    "        self.basic_identities[\"overall_vader_score\"]=overall_vader_score\n",
    "    \n",
    "    def get_transformer_emotion():\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "        emotion_pipeline=pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "            device=device\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0119b2c",
   "metadata": {},
   "source": [
    "#### Create a function including all the procedures of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d78e3732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISTJ : {'neg': 0.058397072171281546, 'neu': 0.7862476634619988, 'pos': 0.12779141333157917, 'compound': 0.20507820073197844}\n",
      "ISFJ : {'neg': 0.05685060523195566, 'neu': 0.7652698584589118, 'pos': 0.1495128194595024, 'compound': 0.2647369412525481}\n",
      "INFJ : {'neg': 0.05889462132969688, 'neu': 0.7744811354874349, 'pos': 0.1395434012172978, 'compound': 0.24172584218193552}\n",
      "INTJ : {'neg': 0.06145981372245033, 'neu': 0.7895860229971304, 'pos': 0.12379708313804853, 'compound': 0.1804486578184271}\n",
      "ISTP : {'neg': 0.06627253823179674, 'neu': 0.778439112609197, 'pos': 0.12218960048873538, 'compound': 0.15897119278824814}\n",
      "ISFP : {'neg': 0.05662993543181892, 'neu': 0.7560949845866368, 'pos': 0.1487719624849228, 'compound': 0.25154817207428026}\n",
      "INFP : {'neg': 0.06324379073988888, 'neu': 0.7643142584722775, 'pos': 0.14190423650167977, 'compound': 0.23163995226385645}\n",
      "INTP : {'neg': 0.06334709693562497, 'neu': 0.7874721200381692, 'pos': 0.11954433977055647, 'compound': 0.16646349430632007}\n",
      "ESTP : {'neg': 0.06301349055478148, 'neu': 0.7792377994002774, 'pos': 0.13593847184957236, 'compound': 0.19699449249946016}\n",
      "ESFP : {'neg': 0.05880721913057419, 'neu': 0.7725976799107831, 'pos': 0.14392423651153577, 'compound': 0.21934569166067377}\n",
      "ENFP : {'neg': 0.058749274424854045, 'neu': 0.7610604728329005, 'pos': 0.16250869452317426, 'compound': 0.298941395906736}\n",
      "ENTP : {'neg': 0.0635766188462074, 'neu': 0.7871272101845609, 'pos': 0.13172136108332372, 'compound': 0.2008135434274433}\n",
      "ESTJ : {'neg': 0.05946899318014615, 'neu': 0.7803324420862635, 'pos': 0.14011525516905207, 'compound': 0.21505243334431612}\n",
      "ESFJ : {'neg': 0.053254435620305636, 'neu': 0.7817250536703254, 'pos': 0.15487390097729922, 'compound': 0.3004873876955675}\n",
      "ENFJ : {'neg': 0.057610693233379716, 'neu': 0.7585769490035248, 'pos': 0.1676042777788771, 'compound': 0.3085231128767869}\n",
      "ENTJ : {'neg': 0.06311361179106713, 'neu': 0.7843797584670696, 'pos': 0.13213333259471774, 'compound': 0.19845929701879472}\n"
     ]
    }
   ],
   "source": [
    "def analyze_data(TYPE):\n",
    "    data=Data_to_Analyze(type=TYPE)\n",
    "    data.remove_url()\n",
    "    # Some features like text readability need to be collected BEFORE the following cleaning procedures\n",
    "    # Otherwise, they are NOT accurate\n",
    "    data.get_sentence_quantity()\n",
    "    data.get_word_count()\n",
    "    data.get_upper_ratio()\n",
    "    data.get_readability()\n",
    "    data.expand_contractions()\n",
    "    data.get_vader_score()\n",
    "    print(data.basic_identities[\"type\"],\":\",data.basic_identities[\"overall_vader_score\"])\n",
    "    # Continue to clean the data\n",
    "    data.tolower()\n",
    "    data.remove_punct()\n",
    "    data.remove_whitespace()\n",
    "    data.totokens()\n",
    "    data.data_to_vec = copy.deepcopy(data.data)\n",
    "    data.remove_stopwords()\n",
    "    data.post_lemmatize()\n",
    "    # Save cleaned data to pickle binary files so that they can be loaded easily in other programs\n",
    "    with open(f\"Data\\\\cleaned_data\\\\{TYPE}_cleaned.pkl\",\"wb\") as f:\n",
    "        pickle.dump(data,f)\n",
    "\n",
    "# Analyze posts from all MBTI types\n",
    "\n",
    "for T in MBTI_types:\n",
    "    analyze_data(T)\n",
    "\n",
    "# analyze_data(\"INFP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009b32f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
