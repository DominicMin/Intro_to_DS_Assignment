{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9da748",
   "metadata": {},
   "source": [
    "**Abstract**: This program uses Regular Expression (`re`) and Natural Language Toolkit (`nltk`) to clean raw post data and collect some features of the data. It uses object-oriented programming (OOP) strategy and creates father class `Data_to_Clean` and derived class `Data_to_Analyze` including various methods to clean and analyze data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bedaa",
   "metadata": {},
   "source": [
    "### Import modules and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b952557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "# Module to load raw data(CSV file)\n",
    "import pandas as pd\n",
    "\n",
    "# Modules for NLP\n",
    "import re # Regular Expression\n",
    "import string\n",
    "from typing import List\n",
    "import nltk # Natural Language Toolkit\n",
    "from nltk.tokenize import word_tokenize # For text tokenization\n",
    "from nltk.corpus import stopwords,wordnet # For stopwords removal\n",
    "# For tokens part-of-speech tagging and lemmatization\n",
    "from nltk import pos_tag \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "my_nltk_path=\"Data\"\n",
    "nltk.data.path.append(my_nltk_path)\n",
    "import textstat # Evaluate text readability\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer # Evaluate text emotion\n",
    "\n",
    "# Modules to read/write external files,etc.\n",
    "import json\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "# Average function\n",
    "def ave(l):\n",
    "    return sum(l)/len(l)\n",
    "\n",
    "# MBTI type dictionary\n",
    "MBTI_types = [\n",
    "    \"ISTJ\", \"ISFJ\", \"INFJ\", \"INTJ\",\n",
    "    \"ISTP\", \"ISFP\", \"INFP\", \"INTP\",\n",
    "    \"ESTP\", \"ESFP\", \"ENFP\", \"ENTP\",\n",
    "    \"ESTJ\", \"ESFJ\", \"ENFJ\", \"ENTJ\"\n",
    "]\n",
    "\n",
    "# Data loading and spliting \n",
    "raw_data=pd.read_csv(\"Data\\\\mbti_1.csv\")\n",
    "for i in raw_data.index:\n",
    "    temp=raw_data.loc[i,\"posts\"]\n",
    "    temp=temp.split(\"|||\")\n",
    "    raw_data.loc[i,\"posts\"]=temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6b340",
   "metadata": {},
   "source": [
    "### Create a class to clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef0258b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_to_Clean:\n",
    "\n",
    "    # Load the contraction map in class\n",
    "    with open(file=\"contractions.json\",mode='r',encoding='utf-8') as f:\n",
    "        contractions_map=json.load(f)\n",
    "    def __init__(self,source=raw_data):\n",
    "        #self.data should be ALL THE POSTS, type:pd.Series\n",
    "        self.data=source\n",
    "        \n",
    "    # Remove URL\n",
    "    def remove_url(self):\n",
    "        def process_remove_url(post):\n",
    "            post_without_url=[]\n",
    "            for sentence in post:\n",
    "                # Use re to scan and substitute\n",
    "                post_without_url.append(\n",
    "                re.sub(\n",
    "                    pattern=r'http\\S+|www\\S+|https\\S+',\n",
    "                    repl='',\n",
    "                    string=sentence,\n",
    "                    flags=re.MULTILINE\n",
    "                    )\n",
    "                )\n",
    "            return post_without_url\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_remove_url)\n",
    "    \n",
    "    # Expand contractions\n",
    "    @staticmethod\n",
    "    def text_expand(original_string,contraction_mapping=contractions_map):\n",
    "        # Compile an re pattern\n",
    "        contractions_pattern = re.compile(\n",
    "            '({})'.format('|'.join(contraction_mapping.keys())),\n",
    "            flags=re.IGNORECASE|re.DOTALL\n",
    "            )\n",
    "        # Map original string to expanded string\n",
    "        def text_mapping(text_matched):\n",
    "            old_text=text_matched.group(0)\n",
    "            new_text=contraction_mapping.get(old_text.lower())\n",
    "            if not new_text:\n",
    "                new_text=contraction_mapping.get(old_text)\n",
    "                if not new_text:\n",
    "                    return old_text\n",
    "            return new_text\n",
    "        # Use re.sub() to scan and substitute\n",
    "        expanded_string=contractions_pattern.sub(\n",
    "            repl=lambda m:text_mapping(m),\n",
    "            string=original_string\n",
    "        )\n",
    "        return expanded_string\n",
    "    # Apply the function to dataset\n",
    "    def expand_contractions(self):\n",
    "        def process_expand_contractions(original_list):\n",
    "            for idx in range(len(original_list)):\n",
    "                original_list[idx]=Data_to_Clean.text_expand(original_list[idx])\n",
    "            return original_list\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(lambda x:process_expand_contractions(x))\n",
    "\n",
    "    # Convert to lower case\n",
    "    def tolower(self):\n",
    "        def process_tolower(post):\n",
    "            return [\n",
    "                sentence.lower() for sentence in post\n",
    "            ]\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_tolower)\n",
    "    \n",
    "    # Remove punctuations\n",
    "    def remove_punct(self):\n",
    "        def process_remove_punct(post):\n",
    "            post_without_punct=[]\n",
    "            for sentence in post:\n",
    "                post_without_punct.append(\n",
    "                    re.sub(\n",
    "                    pattern=r'[^a-zA-Z\\s]',\n",
    "                    repl=' ',\n",
    "                    string=sentence\n",
    "                    )\n",
    "                )\n",
    "            return post_without_punct\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_remove_punct)\n",
    "        \n",
    "    # Remove empty string and whitespace characters\n",
    "    def remove_whitespace(self):\n",
    "        def process_remove_whitespace(post):\n",
    "            return [\n",
    "                sentence for sentence in post if sentence.strip()\n",
    "            ]\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_remove_whitespace)\n",
    "\n",
    "    # Text tokenization\n",
    "    def totokens(self):\n",
    "        def process_totokens(post):\n",
    "            post_totokens=[]\n",
    "            for sentence in post:\n",
    "                tokens=word_tokenize(sentence)\n",
    "                post_totokens.append(tokens)\n",
    "            return post_totokens\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_totokens)\n",
    "    \n",
    "    # Remove stopwords in tokenized text\n",
    "    def remove_stopwords(self):\n",
    "        def process_remove_stopwords(post):\n",
    "            stop_words=set(stopwords.words(\"english\"))\n",
    "            filtered_post=[]\n",
    "            for sentence in post:\n",
    "                filtered_sentence=[]\n",
    "                for word in sentence:\n",
    "                    if word not in stop_words:\n",
    "                        filtered_sentence.append(word)\n",
    "                filtered_post.append(filtered_sentence)\n",
    "            return filtered_post\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_remove_stopwords)\n",
    "\n",
    "    # Lemmatization\n",
    "    def post_lemmatize(self):\n",
    "        def process_lemmatize(post):\n",
    "            # Convert format of part-of-speech tags\n",
    "            def get_wordnet_postag(old_postag):\n",
    "                if old_postag.startswith('J'):  \n",
    "                    return wordnet.ADJ \n",
    "                elif old_postag.startswith('V'):  \n",
    "                    return wordnet.VERB\n",
    "                elif old_postag.startswith('N'):  \n",
    "                    return wordnet.NOUN  \n",
    "                elif old_postag.startswith('R'):  \n",
    "                    return wordnet.ADV  \n",
    "                else:  \n",
    "                    return wordnet.NOUN\n",
    "            lemmatizer=WordNetLemmatizer()\n",
    "            lemmatized_post=[]\n",
    "            for tokens in post:\n",
    "                lemmatized_tokens=[]\n",
    "                # Part of speech tagging\n",
    "                tagged_tokens=pos_tag(tokens)\n",
    "                # Lemmatize tokens\n",
    "                for word,tag in tagged_tokens:\n",
    "                    lemmatized_tokens.append(lemmatizer.lemmatize(word,get_wordnet_postag(tag)))\n",
    "                lemmatized_post.append(lemmatized_tokens)\n",
    "            return lemmatized_post\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_lemmatize)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b717ca2d",
   "metadata": {},
   "source": [
    "### Create a derived class to analysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "977a9c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_to_Analyze(Data_to_Clean):\n",
    "    def __init__(self,type,source=raw_data):\n",
    "        # First initialize an object of father class(Data_to_Clean)\n",
    "        super().__init__(source)\n",
    "        # self.data is of type pd.DataFrame, now specific the MBTI type\n",
    "        self.data=self.data.loc[self.data[\"type\"]==type].reset_index(drop=True)\n",
    "        self.data_to_vec=None\n",
    "        # Store bacic identities of the text\n",
    "        self.basic_identities=pd.Series({\n",
    "\n",
    "            \"type\":type,\n",
    "            # Number of sentences in a post\n",
    "            \"sentence_quantity\":[],\n",
    "            \"ave_sentence_quantity\":None,\n",
    "            # Number of words in a post\n",
    "            \"word_count\":[],\n",
    "            \"ave_word_count\":None,\n",
    "            # Ratio of upper case characters in a post\n",
    "            \"upper_ratio\":[],\n",
    "            \"ave_upper_ratio\":None,\n",
    "            # Two indicators of text readability: Flesch Reading Ease and Gunning Fog Index \n",
    "            \"reading_ease\":[],\n",
    "            \"ave_reading_ease\":None,\n",
    "            \"GF_index\":[],\n",
    "            \"ave_GF_index\":None,\n",
    "            # Overall text emotion indicator\n",
    "            \"overall_vader_score\":None\n",
    "        })\n",
    "\n",
    "    # Design various methods to get identity data\n",
    "\n",
    "    def get_sentence_quantity(self):\n",
    "        for post in self.data[\"posts\"].values:\n",
    "            self.basic_identities[\"sentence_quantity\"].append(len(post))\n",
    "        self.basic_identities[\"ave_sentence_quantity\"]=ave(self.basic_identities[\"sentence_quantity\"])\n",
    "    \n",
    "    def get_word_count(self):\n",
    "        for post in self.data[\"posts\"].values:\n",
    "            ans=0\n",
    "            for sentence in post:\n",
    "                ans+=len(sentence.split(\" \"))\n",
    "            self.basic_identities[\"word_count\"].append(ans)\n",
    "        self.basic_identities[\"ave_word_count\"]=ave(self.basic_identities[\"word_count\"])\n",
    " \n",
    "    def get_upper_ratio(self):\n",
    "        for post in self.data[\"posts\"].values:\n",
    "            char_count=0;upper_count=0\n",
    "            for sentence in post:\n",
    "                for char in sentence:\n",
    "                    if char.isalpha():\n",
    "                        char_count+=1\n",
    "                        if char.isupper():\n",
    "                            upper_count+=1\n",
    "            if char_count!=0:\n",
    "                self.basic_identities[\"upper_ratio\"].append(upper_count/char_count)\n",
    "            else:\n",
    "                continue\n",
    "        self.basic_identities[\"ave_upper_ratio\"]=ave(self.basic_identities[\"upper_ratio\"])\n",
    "    \n",
    "    def get_readability(self):\n",
    "        reading_ease=[];GF_idx=[]\n",
    "        for post in self.data[\"posts\"].values:\n",
    "            concatenated_post=post[0]\n",
    "            for idx in range(1,len(post)):\n",
    "                concatenated_post+=post[idx]\n",
    "            reading_ease.append(\n",
    "                textstat.flesch_reading_ease(concatenated_post)\n",
    "            )\n",
    "            GF_idx.append(\n",
    "                textstat.gunning_fog(concatenated_post)\n",
    "            )\n",
    "        self.basic_identities[\"reading_ease\"]=reading_ease\n",
    "        self.basic_identities[\"ave_reading_ease\"]=ave(self.basic_identities[\"reading_ease\"])\n",
    "        self.basic_identities[\"GF_index\"]=GF_idx\n",
    "        self.basic_identities[\"ave_GF_index\"]=ave(self.basic_identities[\"GF_index\"])\n",
    "    def get_vader_score(self):\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        overall_vader_score={'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "        def process_vader_score(post):\n",
    "            def concatenate_full_post(post):\n",
    "                filtered_post=[sentence for sentence in post if not sentence.isspace()]\n",
    "                return \"\".join(filtered_post)\n",
    "            post_string=concatenate_full_post(post)\n",
    "            scores=analyzer.polarity_scores(post_string)\n",
    "            overall_vader_score[\"neg\"]+=scores[\"neg\"]\n",
    "            overall_vader_score[\"neu\"]+=scores[\"neu\"]\n",
    "            overall_vader_score[\"pos\"]+=scores[\"pos\"]\n",
    "            overall_vader_score[\"compound\"]+=scores[\"compound\"]\n",
    "            return scores\n",
    "        self.data[\"vader_score\"]=self.data[\"posts\"].apply(process_vader_score)\n",
    "        overall_vader_score[\"neg\"]/=len(self.data)\n",
    "        overall_vader_score[\"neu\"]/=len(self.data)\n",
    "        overall_vader_score[\"pos\"]/=len(self.data)\n",
    "        overall_vader_score[\"compound\"]/=len(self.data)\n",
    "        self.basic_identities[\"overall_vader_score\"]=overall_vader_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f051759",
   "metadata": {},
   "source": [
    "#### Test the class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0119b2c",
   "metadata": {},
   "source": [
    "#### Create a function including all the procedures of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d78e3732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                                                                  ISTJ\n",
      "sentence_quantity        [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 47, 3...\n",
      "ave_sentence_quantity                                            48.356098\n",
      "word_count               [936, 1289, 1498, 1392, 1532, 1698, 1921, 1355...\n",
      "ave_word_count                                                 1302.160976\n",
      "upper_ratio              [0.05190708345282478, 0.04737864077669903, 0.0...\n",
      "ave_upper_ratio                                                   0.053204\n",
      "reading_ease             [76.52, 76.93, 77.53, 79.67, 74.9, 73.98, 76.3...\n",
      "ave_reading_ease                                                 73.097854\n",
      "GF_index                 [7.08, 7.67, 6.93, 6.38, 8.22, 8.45, 7.16, 7.3...\n",
      "ave_GF_index                                                      7.710293\n",
      "overall_vader_score      {'neg': 0.07765365853658539, 'neu': 0.77300000...\n",
      "dtype: object\n",
      "type                                                                  ISFJ\n",
      "sentence_quantity        [50, 50, 29, 50, 45, 50, 50, 50, 50, 49, 50, 5...\n",
      "ave_sentence_quantity                                            48.921687\n",
      "word_count               [1703, 1445, 857, 1733, 597, 1677, 1203, 1558,...\n",
      "ave_word_count                                                 1323.307229\n",
      "upper_ratio              [0.0370804631393815, 0.062099223759703004, 0.0...\n",
      "ave_upper_ratio                                                   0.057848\n",
      "reading_ease             [73.58, 67.45, 76.52, 77.94, 70.19, 76.11, 78....\n",
      "ave_reading_ease                                                 73.260964\n",
      "GF_index                 [7.9, 7.75, 7.08, 5.84, 6.34, 7.26, 5.95, 6.92...\n",
      "ave_GF_index                                                      7.436325\n",
      "overall_vader_score      {'neg': 0.07718072289156626, 'neu': 0.75090963...\n",
      "dtype: object\n",
      "type                                                                  INFJ\n",
      "sentence_quantity        [50, 50, 50, 50, 50, 49, 50, 50, 50, 50, 50, 5...\n",
      "ave_sentence_quantity                                             49.05102\n",
      "word_count               [627, 1420, 802, 1559, 1334, 1467, 1620, 1202,...\n",
      "ave_word_count                                                 1364.729932\n",
      "upper_ratio              [0.03197789182787209, 0.049809754410238674, 0....\n",
      "ave_upper_ratio                                                   0.051328\n",
      "reading_ease             [68.26, 76.22, 74.08, 74.79, 75.4, 68.57, 64.4...\n",
      "ave_reading_ease                                                 72.891769\n",
      "GF_index                 [7.79, 7.4, 7.98, 8.16, 7.78, 8.07, 9.11, 8.27...\n",
      "ave_GF_index                                                      7.734755\n",
      "overall_vader_score      {'neg': 0.08033129251700658, 'neu': 0.75533945...\n",
      "dtype: object\n",
      "type                                                                  INTJ\n",
      "sentence_quantity        [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 5...\n",
      "ave_sentence_quantity                                            48.094409\n",
      "word_count               [1150, 1597, 1311, 1678, 1528, 1061, 1220, 126...\n",
      "ave_word_count                                                 1278.379468\n",
      "upper_ratio              [0.05417118093174431, 0.05941057227506627, 0.0...\n",
      "ave_upper_ratio                                                   0.051047\n",
      "reading_ease             [68.77, 77.33, 73.37, 66.84, 73.37, 78.45, 77....\n",
      "ave_reading_ease                                                 70.737938\n",
      "GF_index                 [7.6, 7.16, 8.58, 8.66, 7.9, 5.88, 6.32, 9.19,...\n",
      "ave_GF_index                                                      7.992438\n",
      "overall_vader_score      {'neg': 0.08315582034830424, 'neu': 0.76999725...\n",
      "dtype: object\n",
      "type                                                                  ISTP\n",
      "sentence_quantity        [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 5...\n",
      "ave_sentence_quantity                                             48.95549\n",
      "word_count               [1711, 1085, 1064, 1449, 1136, 1262, 1319, 566...\n",
      "ave_word_count                                                 1248.596439\n",
      "upper_ratio              [0.051025199561746754, 0.04688561721404304, 0....\n",
      "ave_upper_ratio                                                   0.049912\n",
      "reading_ease             [76.52, 71.95, 83.05, 69.07, 85.99, 76.82, 77....\n",
      "ave_reading_ease                                                 74.075549\n",
      "GF_index                 [6.66, 9.1, 7.31, 7.15, 6.2, 7.17, 6.48, 6.86,...\n",
      "ave_GF_index                                                      7.501128\n",
      "overall_vader_score      {'neg': 0.0886468842729971, 'neu': 0.767400593...\n",
      "dtype: object\n",
      "type                                                                  ISFP\n",
      "sentence_quantity        [50, 50, 34, 50, 50, 44, 50, 50, 50, 50, 50, 5...\n",
      "ave_sentence_quantity                                             47.97048\n",
      "word_count               [971, 1295, 1096, 1375, 1414, 1511, 1627, 1726...\n",
      "ave_word_count                                                 1213.630996\n",
      "upper_ratio              [0.05833108290575209, 0.04444841128168511, 0.0...\n",
      "ave_upper_ratio                                                   0.055287\n",
      "reading_ease             [76.72, 65.12, 79.06, 74.49, 78.35, 76.72, 65....\n",
      "ave_reading_ease                                                 73.395867\n",
      "GF_index                 [6.87, 8.68, 7.05, 8.27, 6.32, 7.4, 8.05, 6.88...\n",
      "ave_GF_index                                                      7.594576\n",
      "overall_vader_score      {'neg': 0.07907011070110702, 'neu': 0.75071586...\n",
      "dtype: object\n",
      "type                                                                  INFP\n",
      "sentence_quantity        [50, 50, 28, 50, 48, 50, 50, 50, 50, 50, 42, 5...\n",
      "ave_sentence_quantity                                            49.015284\n",
      "word_count               [1687, 1254, 692, 1201, 1543, 1719, 1540, 1483...\n",
      "ave_word_count                                                 1328.991812\n",
      "upper_ratio              [0.03705767623293396, 0.0456968773800457, 0.04...\n",
      "ave_upper_ratio                                                   0.049654\n",
      "reading_ease             [67.96, 67.55, 78.55, 76.72, 75.71, 76.42, 76....\n",
      "ave_reading_ease                                                 73.241829\n",
      "GF_index                 [7.68, 8.2, 6.64, 6.75, 7.12, 6.7, 9.7, 7.04, ...\n",
      "ave_GF_index                                                       7.73065\n",
      "overall_vader_score      {'neg': 0.08518176855895165, 'neu': 0.74737390...\n",
      "dtype: object\n",
      "type                                                                  INTP\n",
      "sentence_quantity        [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 5...\n",
      "ave_sentence_quantity                                             48.58819\n",
      "word_count               [918, 1315, 942, 1285, 1162, 1224, 1093, 1614,...\n",
      "ave_word_count                                                 1279.489264\n",
      "upper_ratio              [0.04233654876741694, 0.017713365539452495, 0....\n",
      "ave_upper_ratio                                                   0.049601\n",
      "reading_ease             [66.64, 65.42, 66.54, 69.18, 59.3, 68.77, 68.3...\n",
      "ave_reading_ease                                                 71.077753\n",
      "GF_index                 [7.86, 9.13, 8.49, 7.71, 9.29, 8.34, 6.9, 8.12...\n",
      "ave_GF_index                                                      7.961357\n",
      "overall_vader_score      {'neg': 0.08605368098159472, 'neu': 0.76965414...\n",
      "dtype: object\n",
      "type                                                                  ESTP\n",
      "sentence_quantity        [50, 50, 50, 50, 50, 43, 43, 50, 50, 40, 50, 5...\n",
      "ave_sentence_quantity                                            48.730337\n",
      "word_count               [740, 1468, 1089, 1143, 1249, 1594, 895, 1469,...\n",
      "ave_word_count                                                 1250.786517\n",
      "upper_ratio              [0.08960104643557881, 0.06933235509904623, 0.0...\n",
      "ave_upper_ratio                                                   0.058101\n",
      "reading_ease             [70.29, 87.82, 62.31, 76.62, 81.83, 78.14, 77....\n",
      "ave_reading_ease                                                 74.573708\n",
      "GF_index                 [7.12, 4.67, 12.96, 6.6, 7.41, 6.23, 6.55, 7.4...\n",
      "ave_GF_index                                                      7.516292\n",
      "overall_vader_score      {'neg': 0.08247191011235955, 'neu': 0.75807865...\n",
      "dtype: object\n",
      "type                                                                  ESFP\n",
      "sentence_quantity        [53, 50, 50, 50, 50, 50, 39, 50, 50, 50, 50, 5...\n",
      "ave_sentence_quantity                                            46.145833\n",
      "word_count               [922, 296, 1348, 1068, 859, 1188, 1221, 1663, ...\n",
      "ave_word_count                                                 1101.333333\n",
      "upper_ratio              [0.06725297465080186, 0.14542020774315392, 0.0...\n",
      "ave_upper_ratio                                                   0.061799\n",
      "reading_ease             [69.07, 80.88, 85.99, 69.89, 78.55, 74.59, 66....\n",
      "ave_reading_ease                                                 74.248542\n",
      "GF_index                 [7.67, 5.29, 5.95, 7.25, 6.32, 8.19, 8.81, 7.7...\n",
      "ave_GF_index                                                      7.328958\n",
      "overall_vader_score      {'neg': 0.08012500000000002, 'neu': 0.75991666...\n",
      "dtype: object\n",
      "type                                                                  ENFP\n",
      "sentence_quantity        [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 5...\n",
      "ave_sentence_quantity                                            48.546667\n",
      "word_count               [1856, 1508, 1359, 1308, 1145, 1366, 1111, 927...\n",
      "ave_word_count                                                 1345.533333\n",
      "upper_ratio              [0.03661812526509261, 0.04079665917121748, 0.0...\n",
      "ave_upper_ratio                                                   0.056934\n",
      "reading_ease             [82.34, 68.87, 74.9, 75.5, 69.38, 70.02, 69.28...\n",
      "ave_reading_ease                                                 74.426548\n",
      "GF_index                 [7.16, 7.96, 7.57, 7.35, 7.49, 9.48, 7.57, 9.5...\n",
      "ave_GF_index                                                      7.477807\n",
      "overall_vader_score      {'neg': 0.07764444444444449, 'neu': 0.73833185...\n",
      "dtype: object\n",
      "type                                                                  ENTP\n",
      "sentence_quantity        [50, 32, 50, 50, 47, 50, 50, 50, 50, 27, 12, 5...\n",
      "ave_sentence_quantity                                            49.286131\n",
      "word_count               [1243, 549, 1710, 1457, 1708, 1339, 1675, 1003...\n",
      "ave_word_count                                                 1290.583942\n",
      "upper_ratio              [0.06565235514797832, 0.06826666666666667, 0.0...\n",
      "ave_upper_ratio                                                   0.054246\n",
      "reading_ease             [86.81, 69.28, 66.94, 68.57, 82.54, 77.84, 66....\n",
      "ave_reading_ease                                                 71.995796\n",
      "GF_index                 [5.96, 7.24, 8.65, 7.97, 7.45, 5.88, 8.34, 6.6...\n",
      "ave_GF_index                                                      7.929197\n",
      "overall_vader_score      {'neg': 0.08412262773722631, 'neu': 0.76280291...\n",
      "dtype: object\n",
      "type                                                                  ESTJ\n",
      "sentence_quantity        [50, 50, 46, 42, 50, 50, 50, 50, 50, 50, 50, 5...\n",
      "ave_sentence_quantity                                             49.25641\n",
      "word_count               [1213, 879, 1081, 907, 1482, 1364, 1486, 1401,...\n",
      "ave_word_count                                                 1311.025641\n",
      "upper_ratio              [0.08487903225806452, 0.039461883408071746, 0....\n",
      "ave_upper_ratio                                                   0.054576\n",
      "reading_ease             [77.84, 69.35, 77.43, 66.84, 75.91, 68.36, 68....\n",
      "ave_reading_ease                                                 73.004872\n",
      "GF_index                 [6.64, 12.46, 6.35, 8.32, 7.31, 7.33, 7.98, 11...\n",
      "ave_GF_index                                                      7.788718\n",
      "overall_vader_score      {'neg': 0.0830769230769231, 'neu': 0.762846153...\n",
      "dtype: object\n",
      "type                                                                  ESFJ\n",
      "sentence_quantity        [47, 50, 50, 50, 50, 50, 50, 50, 25, 50, 50, 5...\n",
      "ave_sentence_quantity                                            48.047619\n",
      "word_count               [1265, 1109, 1375, 1416, 1593, 1855, 1357, 162...\n",
      "ave_word_count                                                 1372.071429\n",
      "upper_ratio              [0.07004288339799877, 0.07837065269175798, 0.0...\n",
      "ave_upper_ratio                                                   0.062463\n",
      "reading_ease             [77.74, 77.03, 95.98, 78.14, 75.4, 65.62, 77.2...\n",
      "ave_reading_ease                                                 73.908095\n",
      "GF_index                 [6.63, 7.23, 4.44, 6.18, 6.97, 8.5, 6.73, 6.43...\n",
      "ave_GF_index                                                      7.500952\n",
      "overall_vader_score      {'neg': 0.07254761904761906, 'neu': 0.75076190...\n",
      "dtype: object\n",
      "type                                                                  ENFJ\n",
      "sentence_quantity        [43, 50, 50, 50, 49, 38, 50, 46, 50, 50, 50, 5...\n",
      "ave_sentence_quantity                                            48.884211\n",
      "word_count               [853, 1081, 1342, 1788, 1305, 513, 1551, 1516,...\n",
      "ave_word_count                                                 1373.778947\n",
      "upper_ratio              [0.07917031843412212, 0.044793087767166895, 0....\n",
      "ave_upper_ratio                                                   0.053705\n",
      "reading_ease             [66.74, 75.4, 75.3, 76.72, 69.38, 70.8, 66.44,...\n",
      "ave_reading_ease                                                 73.328105\n",
      "GF_index                 [7.52, 7.5, 7.51, 6.57, 7.1, 6.68, 9.18, 7.18,...\n",
      "ave_GF_index                                                      7.652895\n",
      "overall_vader_score      {'neg': 0.07825789473684207, 'neu': 0.73734736...\n",
      "dtype: object\n",
      "type                                                                  ENTJ\n",
      "sentence_quantity        [50, 50, 50, 50, 50, 50, 42, 50, 50, 50, 58, 5...\n",
      "ave_sentence_quantity                                            48.800866\n",
      "word_count               [1036, 1277, 1146, 1795, 1631, 1315, 1120, 130...\n",
      "ave_word_count                                                 1304.549784\n",
      "upper_ratio              [0.04817618719889883, 0.07796741660201707, 0.0...\n",
      "ave_upper_ratio                                                   0.054595\n",
      "reading_ease             [69.28, 75.3, 77.13, 76.72, 64.1, 73.58, 69.28...\n",
      "ave_reading_ease                                                 71.455931\n",
      "GF_index                 [7.76, 7.44, 7.06, 6.42, 10.09, 8.44, 7.28, 7....\n",
      "ave_GF_index                                                      7.807662\n",
      "overall_vader_score      {'neg': 0.08266233766233769, 'neu': 0.76676190...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def analyze_data(TYPE):\n",
    "    data=Data_to_Analyze(type=TYPE)\n",
    "    data.remove_url()\n",
    "    # Some features like text readability need to be collected BEFORE the following cleaning procedures\n",
    "    # Otherwise, they are NOT accurate\n",
    "    data.get_vader_score()\n",
    "    data.get_sentence_quantity()\n",
    "    data.get_word_count()\n",
    "    data.get_upper_ratio()\n",
    "    data.get_readability()\n",
    "    print(data.basic_identities)\n",
    "    # Continue to clean the data\n",
    "    data.expand_contractions()\n",
    "    data.tolower()\n",
    "    data.remove_punct()\n",
    "    data.remove_whitespace()\n",
    "    data.totokens()\n",
    "    data.data_to_vec = copy.deepcopy(data.data)\n",
    "    data.remove_stopwords()\n",
    "    data.post_lemmatize()\n",
    "    # Save cleaned data to pickle binary files so that they can be loaded easily in other programs\n",
    "    with open(f\"Data\\\\cleaned_data\\\\{TYPE}_cleaned.pkl\",\"wb\") as f:\n",
    "        pickle.dump(data,f)\n",
    "\n",
    "# Analyze posts from all MBTI types\n",
    "\n",
    "for T in MBTI_types:\n",
    "    analyze_data(T)\n",
    "\n",
    "#analyze_data(\"INFP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
