{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9da748",
   "metadata": {},
   "source": [
    "**Abstract**: This program uses Regular Expression (`re`) and Natural Language Toolkit (`nltk`) to clean raw post data and collect some features of the data. It uses object-oriented programming (OOP) strategy and creates father class `Data_to_Clean` and derived class `Data_to_Analyze` including various methods to clean and analyze data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bedaa",
   "metadata": {},
   "source": [
    "### Import modules and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b952557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "# Module to load raw data(CSV file)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modules for NLP\n",
    "import re # Regular Expression\n",
    "import string\n",
    "from typing import List\n",
    "import nltk # Natural Language Toolkit\n",
    "from nltk.tokenize import word_tokenize # For text tokenization\n",
    "from nltk.corpus import wordnet # For stopwords removal\n",
    "# For tokens part-of-speech tagging and lemmatization\n",
    "from nltk import pos_tag \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "my_nltk_path=\"Data\"\n",
    "nltk.data.path.append(my_nltk_path)\n",
    "import textstat # Evaluate text readability\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer # Evaluate text emotion\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "stop_words=set()\n",
    "stop_words.update(STOPWORDS)\n",
    "\n",
    "# Modules to read/write external files,etc.\n",
    "import json\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "# Language detection \n",
    "import fasttext\n",
    "lang_model=fasttext.load_model(\"lid.176.bin\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Average function\n",
    "def ave(l):\n",
    "    return sum(l)/len(l)\n",
    "\n",
    "# MBTI type dictionary\n",
    "MBTI_types = [\n",
    "    'istj', 'isfj', 'infj', 'intj', \n",
    "    'istp', 'isfp', 'infp', 'intp', \n",
    "    'estp', 'esfp', 'enfp', 'entp', \n",
    "    'estj', 'esfj', 'enfj', 'entj'\n",
    "    ]\n",
    "\n",
    "# Data loading and spliting \n",
    "raw_data=pd.read_csv(\"Data\\\\twitter_MBTI.csv\",encoding='utf-8')\n",
    "raw_data.drop(columns=\"Unnamed: 0\",inplace=True)\n",
    "raw_data.columns=[\"posts\",\"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "378cfd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Pericles216 @HierBeforeTheAC @Sachinettiyil T...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Hispanthicckk Being you makes you look cute||...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Alshymi Les balles sont rÃ©elles et sont tirÃ©e...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm like entp but idiotic|||Hey boy, do you wa...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@kaeshurr1 Give it to @ZargarShanif ... He has...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Frances Farmer Will Have Her Revenge On Seattl...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ðŸ¤£ðŸ¤­ðŸ¤£ðŸ¤­ðŸ¤£ðŸ¤­ðŸ¤£ðŸ¤­ https://t.co/2a0tICP1yk|||Blind faith...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>proud of this one it goes hard https://t.co/RQ...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@Ieokuras so amazing!|||@hxhrats @ETTUKILLUG w...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@JadMitri Good luck Jad!|||@ElsaYaghi A lawyer...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@Katie_D_M SIN|||@Katie_D_M he wants to take a...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@DeniseKollock Same. I wouldnâ€™t know what to s...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Itâ€™s meâ€¦ Iâ€™m writers ðŸ˜‚ https://t.co/lUB2atb2fK...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Have one busy day and only get on Twitter to t...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@KingKiprich1908 @aryscary @gerrybroek @LeoBlo...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@AnOldWombat @Gunsinaustralia @OMGTheMess @goo...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@ciliyong No. For Byul was \"no!\"|||Ok!\\nHere I...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@FurbabiesRmyGo2 I buy bar soap...but probably...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@HapHapner My spouse doesn't ride bikes any mo...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@IPhantomLordI True. But, Iâ€™m fairly confident...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                posts  type\n",
       "0   @Pericles216 @HierBeforeTheAC @Sachinettiyil T...  intj\n",
       "1   @Hispanthicckk Being you makes you look cute||...  intj\n",
       "2   @Alshymi Les balles sont rÃ©elles et sont tirÃ©e...  intj\n",
       "3   I'm like entp but idiotic|||Hey boy, do you wa...  intj\n",
       "4   @kaeshurr1 Give it to @ZargarShanif ... He has...  intj\n",
       "5   Frances Farmer Will Have Her Revenge On Seattl...  intj\n",
       "6   ðŸ¤£ðŸ¤­ðŸ¤£ðŸ¤­ðŸ¤£ðŸ¤­ðŸ¤£ðŸ¤­ https://t.co/2a0tICP1yk|||Blind faith...  intj\n",
       "7   proud of this one it goes hard https://t.co/RQ...  intj\n",
       "8   @Ieokuras so amazing!|||@hxhrats @ETTUKILLUG w...  intj\n",
       "9   @JadMitri Good luck Jad!|||@ElsaYaghi A lawyer...  intj\n",
       "10  @Katie_D_M SIN|||@Katie_D_M he wants to take a...  intj\n",
       "11  @DeniseKollock Same. I wouldnâ€™t know what to s...  intj\n",
       "12  Itâ€™s meâ€¦ Iâ€™m writers ðŸ˜‚ https://t.co/lUB2atb2fK...  intj\n",
       "13  Have one busy day and only get on Twitter to t...  intj\n",
       "14  @KingKiprich1908 @aryscary @gerrybroek @LeoBlo...  intj\n",
       "15  @AnOldWombat @Gunsinaustralia @OMGTheMess @goo...  intj\n",
       "16  @ciliyong No. For Byul was \"no!\"|||Ok!\\nHere I...  intj\n",
       "17  @FurbabiesRmyGo2 I buy bar soap...but probably...  intj\n",
       "18  @HapHapner My spouse doesn't ride bikes any mo...  intj\n",
       "19  @IPhantomLordI True. But, Iâ€™m fairly confident...  intj"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18c9b9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "infp    1282\n",
       "infj    1057\n",
       "intp     811\n",
       "intj     781\n",
       "enfp     729\n",
       "entp     577\n",
       "enfj     518\n",
       "isfp     367\n",
       "isfj     364\n",
       "istp     327\n",
       "entj     279\n",
       "istj     259\n",
       "esfp     174\n",
       "esfj     105\n",
       "estp     100\n",
       "estj      81\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd0e6c8",
   "metadata": {},
   "source": [
    "#### Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28dd24bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in raw_data.index:\n",
    "    raw_data.loc[i,\"posts\"]=raw_data.loc[i,\"posts\"].split(\"|||\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6b340",
   "metadata": {},
   "source": [
    "### Create a class to clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ef0258b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_to_Clean:\n",
    "\n",
    "    # Load the contraction map in class\n",
    "    with open(file=\"contractions.json\",mode='r',encoding='utf-8') as f:\n",
    "        contractions_map=json.load(f)\n",
    "    def __init__(self,source):\n",
    "        #self.data should be ALL THE POSTS, type:pd.Series\n",
    "        self.data=source\n",
    "    \n",
    "    # Remove \"@Mention\" and \"#Tag\"\n",
    "    def remove_mention_and_tag(self):\n",
    "        def process_removal(post):\n",
    "            post_without_mention=[]\n",
    "            for sentence in post:\n",
    "                # Use re to scan and substitute\n",
    "                post_without_mention.append(\n",
    "                    re.sub(\n",
    "                        pattern=r'@\\w+|#\\w+',\n",
    "                        repl=' ',\n",
    "                        string=sentence\n",
    "                    )\n",
    "                )\n",
    "            return post_without_mention\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_removal)\n",
    "        \n",
    "    # Remove URL\n",
    "    def remove_url(self):\n",
    "        def process_remove_url(post):\n",
    "            post_without_url=[]\n",
    "            for sentence in post:\n",
    "                # Use re to scan and substitute\n",
    "                post_without_url.append(\n",
    "                re.sub(\n",
    "                    pattern=r'http\\S+|www\\S+|https\\S+|\\n',\n",
    "                    repl=' ',\n",
    "                    string=sentence,\n",
    "                    flags=re.MULTILINE\n",
    "                    )\n",
    "                )\n",
    "            return post_without_url\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_remove_url)\n",
    "    \n",
    "    # Remove emoji\n",
    "    def remove_emoji(self):\n",
    "        def process_remove_emoji(post):\n",
    "            post_without_emoji=[]\n",
    "            for sentence in post:\n",
    "                # Use re to scan and substitute\n",
    "                emoji_pattern=re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols and Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"  # Enclosed Characters, etc.\n",
    "        \"\\U0001f926-\\U0001f937\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U00010000-\\U0010ffff\"  # Broader range for some less common emojis\n",
    "        \"]+\", flags=re.UNICODE\n",
    "                )\n",
    "                post_without_emoji.append(\n",
    "                    emoji_pattern.sub(\n",
    "                        repl=' ',\n",
    "                        string=sentence\n",
    "                    )\n",
    "                )\n",
    "            return post_without_emoji\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_remove_emoji)\n",
    "    \n",
    "    # Expand contractions\n",
    "    @staticmethod\n",
    "    def text_expand(original_string, contraction_mapping=contractions_map):\n",
    "\n",
    "        standardized_contraction_map = {k.lower(): v for k, v in contraction_mapping.items()}\n",
    "\n",
    "        sorted_contractions = sorted(\n",
    "            standardized_contraction_map.items(),\n",
    "            key=lambda item: len(item[0]),\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        pattern_parts = []\n",
    "        for contraction, _ in sorted_contractions:\n",
    "            pattern_parts.append(r'\\b' + re.escape(contraction) + r'\\b')\n",
    "\n",
    "        if not pattern_parts:\n",
    "            return original_string\n",
    "\n",
    "        contractions_pattern = re.compile(\n",
    "            '({})'.format('|'.join(pattern_parts)),\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        def text_mapping(match_obj):\n",
    "            old_text = match_obj.group(0)\n",
    "            new_text = standardized_contraction_map.get(old_text.lower())\n",
    "            \n",
    "            if new_text:\n",
    "                return new_text + \" \"\n",
    "            else:\n",
    "                return old_text + \" \"\n",
    "        \n",
    "        expanded_string = contractions_pattern.sub(\n",
    "            repl=text_mapping,\n",
    "            string=original_string\n",
    "        )\n",
    "        final_result = expanded_string.strip()\n",
    "        return final_result\n",
    "    # Apply the function to dataset\n",
    "    def expand_contractions(self):\n",
    "        def process_expand_contractions(original_list):\n",
    "            for idx in range(len(original_list)):\n",
    "                original_list[idx]=Data_to_Clean.text_expand(original_list[idx])\n",
    "            return original_list\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(lambda x:process_expand_contractions(x))\n",
    "\n",
    "    # Convert to lower case\n",
    "    def tolower(self):\n",
    "        def process_tolower(post):\n",
    "            return [\n",
    "                sentence.lower() for sentence in post\n",
    "            ]\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_tolower)\n",
    "\n",
    "    # Remove punctuations\n",
    "    def remove_punct(self):\n",
    "        def process_remove_punct(post):\n",
    "            post_without_punct=[]\n",
    "            for sentence in post:\n",
    "                post_without_punct.append(\n",
    "                    re.sub(\n",
    "                    pattern=r'[^a-zA-Z\\s]',\n",
    "                    repl=' ',\n",
    "                    string=sentence\n",
    "                    )\n",
    "                )\n",
    "            return post_without_punct\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_remove_punct)\n",
    "        \n",
    "    # Remove empty string and whitespace characters\n",
    "    def remove_whitespace(self):\n",
    "        def process_remove_whitespace(post):\n",
    "            result=[sentence for sentence in post if sentence.strip()]\n",
    "            return result\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_remove_whitespace)\n",
    "\n",
    "    # Text tokenization\n",
    "    def totokens(self):\n",
    "        def process_totokens(post):\n",
    "            post_totokens=[]\n",
    "            for sentence in post:\n",
    "                tokens=word_tokenize(sentence)\n",
    "                post_totokens.append(tokens)\n",
    "                # post_totokens.extend(['@SENTENCE-END'])\n",
    "            return post_totokens\n",
    "        # here all posts are flatten\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_totokens)\n",
    "    \n",
    "    \n",
    "    # Remove stopwords in tokenized text\n",
    "    def remove_stopwords(self,stop_words_set):\n",
    "        def process_remove_stopwords(post):\n",
    "            filtered_post=[]\n",
    "            for sentence in post:\n",
    "                if isinstance(sentence,list):\n",
    "                    filtered_sentence=[]\n",
    "                    for word in sentence:\n",
    "                        if word not in stop_words_set:\n",
    "                            filtered_sentence.append(word)\n",
    "                    filtered_post.append(filtered_sentence)\n",
    "                else:\n",
    "                    if sentence not in stop_words_set:\n",
    "                        filtered_post.append(sentence)\n",
    "            return filtered_post\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_remove_stopwords)\n",
    "\n",
    "    # Lemmatization\n",
    "    def post_lemmatize(self):\n",
    "        def process_lemmatize(post):\n",
    "            # Convert format of part-of-speech tags\n",
    "            def get_wordnet_postag(old_postag):\n",
    "                if old_postag.startswith('J'):  \n",
    "                    return wordnet.ADJ \n",
    "                elif old_postag.startswith('V'):  \n",
    "                    return wordnet.VERB\n",
    "                elif old_postag.startswith('N'):  \n",
    "                    return wordnet.NOUN  \n",
    "                elif old_postag.startswith('R'):  \n",
    "                    return wordnet.ADV  \n",
    "                else:  \n",
    "                    return wordnet.NOUN\n",
    "            lemmatizer=WordNetLemmatizer()\n",
    "            lemmatized_post=[]\n",
    "            for tokens in post:\n",
    "                lemmatized_tokens=[]\n",
    "                # Part of speech tagging\n",
    "                tagged_tokens=pos_tag(tokens)\n",
    "                # Lemmatize tokens\n",
    "                for word,tag in tagged_tokens:\n",
    "                    lemmatized_tokens.append(lemmatizer.lemmatize(word,get_wordnet_postag(tag)))\n",
    "                lemmatized_post.append(lemmatized_tokens)\n",
    "            return lemmatized_post\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_lemmatize)\n",
    "\n",
    "    def concatenate_post(self):\n",
    "        def process_concatenate_post(post):\n",
    "            complete_post=[]\n",
    "            for sentence in post:\n",
    "                if sentence:\n",
    "                    complete_post.extend(sentence)\n",
    "            return complete_post\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_concatenate_post)\n",
    "\n",
    "    def drop_non_english(self,level):\n",
    "        '''\n",
    "        To enhance prediction accuracy:\n",
    "        - sentence must be long\n",
    "        - remain English punctuation\n",
    "        - try to convert to lower case and try again\n",
    "        '''\n",
    "        def process_drop(post):\n",
    "            filtered_post=[]\n",
    "            for sentence in post:\n",
    "                normalized_sentence=re.sub(r'\\s+', ' ', sentence)\n",
    "                if len(normalized_sentence.split())<6:\n",
    "                    filtered_post.append(normalized_sentence)\n",
    "                    # for very short sentence we won't predict\n",
    "                else:\n",
    "                    lang=lang_model.predict(normalized_sentence)\n",
    "                    if lang[0][0]=='__label__en' and lang[1][0]>level:\n",
    "                        filtered_post.append(normalized_sentence)\n",
    "                    else:\n",
    "                        lang=lang_model.predict(normalized_sentence.lower())\n",
    "                        if lang[0][0]=='__label__en' and lang[1][0]>level:\n",
    "                            filtered_post.append(normalized_sentence)\n",
    "            return filtered_post\n",
    "        self.data[\"posts\"]=self.data[\"posts\"].apply(process_drop)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b717ca2d",
   "metadata": {},
   "source": [
    "### Create a derived class to analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "977a9c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_to_Analyze(Data_to_Clean):\n",
    "    def __init__(self,type,source):\n",
    "        # First initialize an object of father class(Data_to_Clean)\n",
    "        super().__init__(source)\n",
    "        # self.data is of type pd.DataFrame, now specific the MBTI type\n",
    "        self.data=self.data.loc[self.data[\"type\"]==type].reset_index(drop=True)\n",
    "        # Store bacic identities of the text\n",
    "        self.basic_identities=pd.Series({\n",
    "\n",
    "            \"type\":type,\n",
    "            # Number of sentences in a post\n",
    "            \"sentence_quantity\":[],\n",
    "            \"ave_sentence_quantity\":None,\n",
    "            # Number of words in a post\n",
    "            \"word_count\":[],\n",
    "            \"ave_word_count\":None,\n",
    "            # Ratio of upper case characters in a post\n",
    "            \"upper_ratio\":[],\n",
    "            \"ave_upper_ratio\":None,\n",
    "            # Two indicators of text readability: Flesch Reading Ease and Gunning Fog Index \n",
    "            \"reading_ease\":[],\n",
    "            \"ave_reading_ease\":None,\n",
    "            \"GF_index\":[],\n",
    "            \"ave_GF_index\":None,\n",
    "            # Overall text emotion indicator\n",
    "            \"overall_vader_score\":None\n",
    "        })\n",
    "        self.locations=None\n",
    "\n",
    "    # Design various methods to get identity data\n",
    "\n",
    "    def get_sentence_quantity(self):\n",
    "        for post in self.data[\"posts\"].values:\n",
    "            self.basic_identities[\"sentence_quantity\"].append(len(post))\n",
    "        self.basic_identities[\"ave_sentence_quantity\"]=ave(self.basic_identities[\"sentence_quantity\"])\n",
    "    \n",
    "    def get_word_count(self):\n",
    "        for post in self.data[\"posts\"].values:\n",
    "            ans=0\n",
    "            for sentence in post:\n",
    "                ans+=len(sentence.split(\" \"))\n",
    "            self.basic_identities[\"word_count\"].append(ans)\n",
    "        self.basic_identities[\"ave_word_count\"]=ave(self.basic_identities[\"word_count\"])\n",
    " \n",
    "    def get_upper_ratio(self):\n",
    "        for post in self.data[\"posts\"].values:\n",
    "            char_count=0;upper_count=0\n",
    "            for sentence in post:\n",
    "                for char in sentence:\n",
    "                    if char.isalpha():\n",
    "                        char_count+=1\n",
    "                        if char.isupper():\n",
    "                            upper_count+=1\n",
    "            if char_count!=0:\n",
    "                self.basic_identities[\"upper_ratio\"].append(upper_count/char_count)\n",
    "            else:\n",
    "                continue\n",
    "        self.basic_identities[\"ave_upper_ratio\"]=ave(self.basic_identities[\"upper_ratio\"])\n",
    "    \n",
    "    def get_readability(self):\n",
    "        reading_ease=[];GF_idx=[]\n",
    "        for post in self.data[\"posts\"].values:\n",
    "            concatenated_post=post[0]\n",
    "            for idx in range(1,len(post)):\n",
    "                concatenated_post+=post[idx]\n",
    "            reading_ease.append(\n",
    "                textstat.flesch_reading_ease(concatenated_post)\n",
    "            )\n",
    "            GF_idx.append(\n",
    "                textstat.gunning_fog(concatenated_post)\n",
    "            )\n",
    "        self.basic_identities[\"reading_ease\"]=reading_ease\n",
    "        self.basic_identities[\"ave_reading_ease\"]=ave(self.basic_identities[\"reading_ease\"])\n",
    "        self.basic_identities[\"GF_index\"]=GF_idx\n",
    "        self.basic_identities[\"ave_GF_index\"]=ave(self.basic_identities[\"GF_index\"])\n",
    "    \n",
    "    @staticmethod\n",
    "    def concatenate_full_post(post):\n",
    "                filtered_post=[sentence for sentence in post if not sentence.isspace()]\n",
    "                return \"\".join(filtered_post)\n",
    "    def get_vader_score(self):\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        overall_vader_score={'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "        def addup_score_dict(new_dict,base_dict):\n",
    "            for key in base_dict.keys():\n",
    "                base_dict[key]+=new_dict[key]\n",
    "        def ave_score_dict(base_dict,n):\n",
    "            for key in base_dict.keys():\n",
    "                base_dict[key]/=n\n",
    "        def process_vader_score(post):\n",
    "            post_vader_score={'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "            for sentence in post:\n",
    "               addup_score_dict(analyzer.polarity_scores(sentence),base_dict=post_vader_score) \n",
    "            ave_score_dict(base_dict=post_vader_score,n=len(post))\n",
    "            addup_score_dict(new_dict=post_vader_score,base_dict=overall_vader_score)\n",
    "            return post_vader_score\n",
    "        self.data[\"vader_score\"]=self.data[\"posts\"].apply(process_vader_score)\n",
    "        ave_score_dict(overall_vader_score,len(self.data[\"posts\"]))\n",
    "        self.basic_identities[\"overall_vader_score\"]=overall_vader_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cce39d",
   "metadata": {},
   "source": [
    "Since the dataset contains non-English texts that severely infulence the quality of LDA model, we need to filter them. We need to **find a balanced filter level** so that most of the non-English text can be filtered while enough amount of English text still remains.\n",
    "\n",
    "First we need to check the identity of `fasttext` language prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1f1b50",
   "metadata": {},
   "source": [
    "- For a single word or very short sentence, the model is not reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5a52264f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__en',), array([0.30061391]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str=\"hello\"\n",
    "lang_model.predict(test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b89aca",
   "metadata": {},
   "source": [
    "- The model is case sensitive. For sentence in lower case it's more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a9ec6906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__en',), array([0.35276151]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str=\"HELLO, HOW ARE YOU?\"\n",
    "lang_model.predict(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0efba718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__en',), array([0.99444604]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str=\"hello, how are you?\"\n",
    "lang_model.predict(test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf134f3",
   "metadata": {},
   "source": [
    "- Check the amount of remaining sentences at each filter level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0d7fe434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# result={}\n",
    "# for i in np.arange(0,1,0.15):\n",
    "#     result[f\"{i:.2f}\"]=0\n",
    "#     infp=Data_to_Analyze(type='infp',source=raw_data)\n",
    "#     infp.remove_mention_and_tag()\n",
    "#     infp.remove_emoji()\n",
    "#     infp.remove_url()\n",
    "#     infp.remove_whitespace()\n",
    "#     infp.drop_non_english(i)\n",
    "#     for j in infp.data.index:\n",
    "#         result[f\"{i:.2f}\"]+=len(infp.data.loc[j,\"posts\"])\n",
    "# result=pd.Series(result)\n",
    "# plt.plot(result.index,result.values,label=\"Remaining Sentence Quantity\")\n",
    "# plt.xlabel(\"Filter Level\")\n",
    "# plt.ylabel(\"Sentence Quantity\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84b058",
   "metadata": {},
   "source": [
    "Finally, we choose filter level at 0.75, which means if the confidence level of the model>0.75, the sentence remains.\n",
    "\n",
    "And for short sentence (consists of less that 6 words), we directly keep it since the model is not reliable to detect language of such short sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0119b2c",
   "metadata": {},
   "source": [
    "### Construct data clean pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaed8c6",
   "metadata": {},
   "source": [
    "Since we need to **customize stopword** in order to enhance LDA model coherence, we divide data cleaning pipeline into two phrases to **save time**\n",
    "\n",
    "Firstly, we will do all the cleaning process **without stopwords removal** and save the variables in pickle files. When optimizing LDA model, we **will not repeat this phrase**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d78e3732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd5626fa22a4c37a01c672d1e8986db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Analyze posts from all MBTI types\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m T \u001b[38;5;129;01min\u001b[39;00m tqdm(MBTI_types):\n\u001b[1;32m---> 31\u001b[0m     \u001b[43manalyze_data_p1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 12\u001b[0m, in \u001b[0;36manalyze_data_p1\u001b[1;34m(TYPE)\u001b[0m\n\u001b[0;32m     10\u001b[0m data\u001b[38;5;241m.\u001b[39mget_upper_ratio()\n\u001b[0;32m     11\u001b[0m data\u001b[38;5;241m.\u001b[39mget_readability()\n\u001b[1;32m---> 12\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vader_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Continue to clean the data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m data\u001b[38;5;241m.\u001b[39mremove_emoji()\n",
      "Cell \u001b[1;32mIn[27], line 97\u001b[0m, in \u001b[0;36mData_to_Analyze.get_vader_score\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m     addup_score_dict(new_dict\u001b[38;5;241m=\u001b[39mpost_vader_score,base_dict\u001b[38;5;241m=\u001b[39moverall_vader_score)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m post_vader_score\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvader_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mposts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_vader_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m ave_score_dict(overall_vader_score,\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposts\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_identities[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_vader_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39moverall_vader_score\n",
      "File \u001b[1;32mc:\\Users\\DominicMin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DominicMin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DominicMin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\DominicMin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DominicMin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[27], line 93\u001b[0m, in \u001b[0;36mData_to_Analyze.get_vader_score.<locals>.process_vader_score\u001b[1;34m(post)\u001b[0m\n\u001b[0;32m     91\u001b[0m post_vader_score\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneu\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m post:\n\u001b[1;32m---> 93\u001b[0m    addup_score_dict(\u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolarity_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m,base_dict\u001b[38;5;241m=\u001b[39mpost_vader_score) \n\u001b[0;32m     94\u001b[0m ave_score_dict(base_dict\u001b[38;5;241m=\u001b[39mpost_vader_score,n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(post))\n\u001b[0;32m     95\u001b[0m addup_score_dict(new_dict\u001b[38;5;241m=\u001b[39mpost_vader_score,base_dict\u001b[38;5;241m=\u001b[39moverall_vader_score)\n",
      "File \u001b[1;32mc:\\Users\\DominicMin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\vaderSentiment\\vaderSentiment.py:269\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.polarity_scores\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    266\u001b[0m         sentiments\u001b[38;5;241m.\u001b[39mappend(valence)\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m     sentiments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentiment_valence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentitext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentiments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m sentiments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_but_check(words_and_emoticons, sentiments)\n\u001b[0;32m    273\u001b[0m valence_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_valence(sentiments, text)\n",
      "File \u001b[1;32mc:\\Users\\DominicMin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\vaderSentiment\\vaderSentiment.py:316\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.sentiment_valence\u001b[1;34m(self, valence, sentitext, item, i, sentiments)\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    314\u001b[0m                 valence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_special_idioms_check(valence, words_and_emoticons, i)\n\u001b[1;32m--> 316\u001b[0m     valence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_least_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwords_and_emoticons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m sentiments\u001b[38;5;241m.\u001b[39mappend(valence)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sentiments\n",
      "File \u001b[1;32mc:\\Users\\DominicMin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\vaderSentiment\\vaderSentiment.py:320\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer._least_check\u001b[1;34m(self, valence, words_and_emoticons, i)\u001b[0m\n\u001b[0;32m    317\u001b[0m     sentiments\u001b[38;5;241m.\u001b[39mappend(valence)\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentiments\n\u001b[1;32m--> 320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_least_check\u001b[39m(\u001b[38;5;28mself\u001b[39m, valence, words_and_emoticons, i):\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# check for negation case using \"least\"\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m words_and_emoticons[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon \\\n\u001b[0;32m    323\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m words_and_emoticons[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleast\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m words_and_emoticons[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m words_and_emoticons[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvery\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def analyze_data_p1(TYPE):\n",
    "    data=Data_to_Analyze(type=TYPE,source=raw_data)\n",
    "    data.remove_url()\n",
    "    data.remove_mention_and_tag()\n",
    "\n",
    "    # Some features like text readability need to be collected BEFORE the following cleaning procedures\n",
    "    # Otherwise, they are NOT accurate\n",
    "    data.get_sentence_quantity()\n",
    "    data.get_word_count()\n",
    "    data.get_upper_ratio()\n",
    "    data.get_readability()\n",
    "    data.get_vader_score()\n",
    "\n",
    "    # Continue to clean the data\n",
    "    data.remove_emoji()\n",
    "    data.remove_whitespace()\n",
    "    data.drop_non_english(0.75)\n",
    "    data.expand_contractions()\n",
    "    data.tolower()\n",
    "    data.remove_punct()\n",
    "    data.remove_whitespace()\n",
    "    data.totokens()\n",
    "    data.post_lemmatize()\n",
    "    \n",
    "    with open(f\"Data\\\\cleaned_data\\\\p1\\\\{TYPE}_cleaned.pkl\",\"wb\") as f:\n",
    "        pickle.dump(data,f)\n",
    "\n",
    "# Analyze posts from all MBTI types\n",
    "\n",
    "for T in tqdm(MBTI_types):\n",
    "    analyze_data_p1(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d41d817",
   "metadata": {},
   "source": [
    "#### Add custom stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10656c29",
   "metadata": {},
   "source": [
    "In phrase 2, we will load custom stopwords from external `json` file and load cleaned data after **phrase 1** from `pickle` file.\n",
    "\n",
    "The stopwords will be updated every time after training an LDA model. We will **repeat the loop**: *remove stopwords->train LDA model->update stopwords->remove stopwords...* **until LDA result is good enough**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c9e0bddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'tr',\n",
       " 'bout',\n",
       " 'lip',\n",
       " 'hii',\n",
       " 'dong',\n",
       " 'okie',\n",
       " 'exactly',\n",
       " 'ww',\n",
       " 'detail',\n",
       " 'mi',\n",
       " 'during',\n",
       " 'bo',\n",
       " 'hala',\n",
       " 'seeming',\n",
       " 'ever',\n",
       " 'full',\n",
       " 'if',\n",
       " 'physically',\n",
       " 'tell',\n",
       " 'from',\n",
       " 'us',\n",
       " 'hai',\n",
       " 'ew',\n",
       " 'omgggg',\n",
       " 'alright',\n",
       " 'ahhhh',\n",
       " 'third',\n",
       " 'definitely',\n",
       " 'fairly',\n",
       " 'mt',\n",
       " 'bbl',\n",
       " 'bae',\n",
       " 'casually',\n",
       " 'eng',\n",
       " 'off',\n",
       " 'uu',\n",
       " 'hun',\n",
       " 'ironically',\n",
       " 'hereby',\n",
       " 'nevertheless',\n",
       " 'night',\n",
       " 'bgt',\n",
       " 'regularly',\n",
       " 'much',\n",
       " 'yeahhh',\n",
       " 'be',\n",
       " 'itself',\n",
       " 'keep',\n",
       " 'really',\n",
       " 'essentially',\n",
       " 'hence',\n",
       " 'run',\n",
       " 'two',\n",
       " 'what',\n",
       " 'bill',\n",
       " 'quite',\n",
       " 'cutee',\n",
       " 'certainly',\n",
       " 'entire',\n",
       " 'twenty',\n",
       " 'yoooo',\n",
       " 'between',\n",
       " 'whyyy',\n",
       " 'ik',\n",
       " 'imma',\n",
       " 'behind',\n",
       " 'sf',\n",
       " 'he',\n",
       " 'bro',\n",
       " 'amount',\n",
       " 'bub',\n",
       " 'welcome',\n",
       " 'km',\n",
       " 'yep',\n",
       " 'ne',\n",
       " 'mine',\n",
       " 'beautiful',\n",
       " 'using',\n",
       " 'play',\n",
       " 'lang',\n",
       " 'ck',\n",
       " 'ugly',\n",
       " 'bye',\n",
       " 'join',\n",
       " 'nice',\n",
       " 'nah',\n",
       " 'outside',\n",
       " 'think',\n",
       " 'kai',\n",
       " 'yess',\n",
       " 'sometimes',\n",
       " 'thus',\n",
       " 'face',\n",
       " 'br',\n",
       " 'yikes',\n",
       " 'ia',\n",
       " 'ny',\n",
       " 'name',\n",
       " 'im',\n",
       " 'serious',\n",
       " 'cus',\n",
       " 'perhaps',\n",
       " 'our',\n",
       " 'but',\n",
       " 'ooo',\n",
       " 'isa',\n",
       " 'whoa',\n",
       " 'seems',\n",
       " 'well',\n",
       " 'brown',\n",
       " 'bp',\n",
       " 'hru',\n",
       " 'for',\n",
       " 'dia',\n",
       " 'par',\n",
       " 'ddd',\n",
       " 'dp',\n",
       " 'oooh',\n",
       " 'tag',\n",
       " 'whe',\n",
       " 'whence',\n",
       " 'ka',\n",
       " 'besides',\n",
       " 'ohh',\n",
       " 'indeed',\n",
       " 'peo',\n",
       " 'afternoon',\n",
       " 'morning',\n",
       " 'youuuu',\n",
       " 'jp',\n",
       " 'empty',\n",
       " 'together',\n",
       " 'proud',\n",
       " 'ako',\n",
       " 'unfollowed',\n",
       " 'aaaa',\n",
       " 'around',\n",
       " 'no',\n",
       " 'thoughtful',\n",
       " 'had',\n",
       " 'nicely',\n",
       " 'cw',\n",
       " 'genuinely',\n",
       " 'cutie',\n",
       " 'plenty',\n",
       " 'am',\n",
       " 'inc',\n",
       " 'bestie',\n",
       " 'one',\n",
       " 'late',\n",
       " 'ca',\n",
       " 'esp',\n",
       " 'hear',\n",
       " 'good',\n",
       " 'interact',\n",
       " 'sunday',\n",
       " 'gray',\n",
       " 'autometically',\n",
       " 'themselves',\n",
       " 'easily',\n",
       " 'nahh',\n",
       " 'perfectly',\n",
       " 'everybody',\n",
       " 'is',\n",
       " 'give',\n",
       " 'how',\n",
       " 'st',\n",
       " 'sign',\n",
       " 'ss',\n",
       " 'might',\n",
       " 'come',\n",
       " 'mmm',\n",
       " 'oki',\n",
       " 'tiny',\n",
       " 'left',\n",
       " 'whole',\n",
       " 'fc',\n",
       " 'mf',\n",
       " 'twt',\n",
       " 'cuteeee',\n",
       " 'yours',\n",
       " 'aaaaaaaa',\n",
       " 'take',\n",
       " 'weekend',\n",
       " 'wherever',\n",
       " 'such',\n",
       " 'ly',\n",
       " 'na',\n",
       " 'helppp',\n",
       " 'will',\n",
       " 'eu',\n",
       " 'herein',\n",
       " 'goooo',\n",
       " 'like',\n",
       " 'thence',\n",
       " 'potentially',\n",
       " 'show',\n",
       " 'kaya',\n",
       " 'clearly',\n",
       " 'uh',\n",
       " 'cute',\n",
       " 'ou',\n",
       " 'sameee',\n",
       " 'above',\n",
       " 'heavily',\n",
       " 'head',\n",
       " 'youu',\n",
       " 'was',\n",
       " 'thr',\n",
       " 'tru',\n",
       " 'pala',\n",
       " 'in',\n",
       " 'lovely',\n",
       " 'tt',\n",
       " 'get',\n",
       " 'nine',\n",
       " 'umm',\n",
       " 'formerly',\n",
       " 'also',\n",
       " 'nor',\n",
       " 'wordle',\n",
       " 'mad',\n",
       " 'unnie',\n",
       " 'incredibly',\n",
       " 'grey',\n",
       " 'ear',\n",
       " 'can',\n",
       " 'may',\n",
       " 'tf',\n",
       " 'dang',\n",
       " 'gago',\n",
       " 'aaaaaaa',\n",
       " 'nu',\n",
       " 'twelve',\n",
       " 'totally',\n",
       " 'yessssss',\n",
       " 'nga',\n",
       " 'kan',\n",
       " 'me',\n",
       " 'properly',\n",
       " 'hiii',\n",
       " 'rly',\n",
       " 'over',\n",
       " 'str',\n",
       " 'large',\n",
       " 'lah',\n",
       " 'his',\n",
       " 'meeee',\n",
       " 'wdym',\n",
       " 'back',\n",
       " 'walk',\n",
       " 'ayo',\n",
       " 'yew',\n",
       " 'thursday',\n",
       " 'tu',\n",
       " 'everyone',\n",
       " 'ba',\n",
       " 'noooooo',\n",
       " 'and',\n",
       " 'mag',\n",
       " 'prolly',\n",
       " 'pl',\n",
       " 'front',\n",
       " 'cry',\n",
       " 'tooo',\n",
       " 'anyhow',\n",
       " 'initially',\n",
       " 'teh',\n",
       " 'highly',\n",
       " 'move',\n",
       " 'ahhh',\n",
       " 'gurl',\n",
       " 'recently',\n",
       " 'sc',\n",
       " 'dk',\n",
       " 'hereafter',\n",
       " 'gladly',\n",
       " 'successfully',\n",
       " 'per',\n",
       " 'through',\n",
       " 'dude',\n",
       " 'sl',\n",
       " 'fifteen',\n",
       " 'mot',\n",
       " 'hundred',\n",
       " 'everything',\n",
       " 'ff',\n",
       " 'er',\n",
       " 'jus',\n",
       " 'fifty',\n",
       " 'up',\n",
       " 'say',\n",
       " 'mp',\n",
       " 'sincere',\n",
       " 'system',\n",
       " 'mx',\n",
       " 'sht',\n",
       " 'probs',\n",
       " 'intentionally',\n",
       " 'herself',\n",
       " 'others',\n",
       " 'ay',\n",
       " 'latter',\n",
       " 'priv',\n",
       " 'alr',\n",
       " 'en',\n",
       " 'cos',\n",
       " 'opportunity',\n",
       " 'yayyyy',\n",
       " 'smth',\n",
       " 'frfr',\n",
       " 'whether',\n",
       " 'mouth',\n",
       " 'je',\n",
       " 'early',\n",
       " 'too',\n",
       " 'watch',\n",
       " 'thereupon',\n",
       " 'hoy',\n",
       " 'phew',\n",
       " 'nooooooo',\n",
       " 'latterly',\n",
       " 'con',\n",
       " 'legitimately',\n",
       " 'leg',\n",
       " 'here',\n",
       " 'talaga',\n",
       " 'loo',\n",
       " 'sad',\n",
       " 'whenever',\n",
       " 'actively',\n",
       " 'q',\n",
       " 'somebody',\n",
       " 'oml',\n",
       " 'qu',\n",
       " 'still',\n",
       " 'yayy',\n",
       " 'yasss',\n",
       " 'necessarily',\n",
       " 'became',\n",
       " 'three',\n",
       " 'noone',\n",
       " 'bs',\n",
       " 'quickly',\n",
       " 'pp',\n",
       " 'ag',\n",
       " 'loml',\n",
       " 'even',\n",
       " 'work',\n",
       " 'fill',\n",
       " 'block',\n",
       " 'which',\n",
       " 'doing',\n",
       " 'hq',\n",
       " 'five',\n",
       " 'gm',\n",
       " 'dah',\n",
       " 'tomorrow',\n",
       " 'were',\n",
       " 'elsewhere',\n",
       " 'hmmm',\n",
       " 'throughout',\n",
       " 'alone',\n",
       " 'tyyy',\n",
       " 'chi',\n",
       " 'otherwise',\n",
       " 'about',\n",
       " 'ah',\n",
       " 'ni',\n",
       " 'we',\n",
       " 'finger',\n",
       " 'overwhelming',\n",
       " 'nobody',\n",
       " 'yung',\n",
       " 'mostly',\n",
       " 'mak',\n",
       " 'f',\n",
       " 'whoever',\n",
       " 'dear',\n",
       " 'sta',\n",
       " 'yay',\n",
       " 'awesome',\n",
       " 'via',\n",
       " 'interest',\n",
       " 'oppa',\n",
       " 'actually',\n",
       " 'objectively',\n",
       " 'yellow',\n",
       " 'doesn',\n",
       " 'that',\n",
       " 'ju',\n",
       " 'll',\n",
       " 'rlly',\n",
       " 'crazy',\n",
       " 'neither',\n",
       " 'wooooo',\n",
       " 'perfect',\n",
       " 'several',\n",
       " 'before',\n",
       " 'whatever',\n",
       " 'uhhh',\n",
       " 'they',\n",
       " 'girlies',\n",
       " 'cant',\n",
       " 'yayyy',\n",
       " 'istg',\n",
       " 'heyyy',\n",
       " 'hardly',\n",
       " 'j',\n",
       " 'nowhere',\n",
       " 'last',\n",
       " 'do',\n",
       " 'yourself',\n",
       " 'cannot',\n",
       " 'aaah',\n",
       " 'hers',\n",
       " 'um',\n",
       " 'youuu',\n",
       " 'former',\n",
       " 'usual',\n",
       " 'fav',\n",
       " 'consistently',\n",
       " 'it',\n",
       " 'fb',\n",
       " 'someone',\n",
       " 'anything',\n",
       " 'ohhh',\n",
       " 'regarding',\n",
       " 'normally',\n",
       " 'ang',\n",
       " 'hav',\n",
       " 'hw',\n",
       " 'mu',\n",
       " 'eid',\n",
       " 've',\n",
       " 'z',\n",
       " 'lmaooooo',\n",
       " 'pd',\n",
       " 'ng',\n",
       " 'himself',\n",
       " 'lmfaooo',\n",
       " 'yea',\n",
       " 'barely',\n",
       " 'six',\n",
       " 'heheh',\n",
       " 'fire',\n",
       " 'aw',\n",
       " 'rec',\n",
       " 'heh',\n",
       " 'while',\n",
       " 'lmaooo',\n",
       " 'pr',\n",
       " 'usually',\n",
       " 'foot',\n",
       " 'meh',\n",
       " 'ap',\n",
       " 'super',\n",
       " 'yeahh',\n",
       " 'please',\n",
       " 're',\n",
       " 'been',\n",
       " 'si',\n",
       " 'ser',\n",
       " 'urself',\n",
       " 'right',\n",
       " 'thanks',\n",
       " 'plsss',\n",
       " 'aku',\n",
       " 'many',\n",
       " 'amo',\n",
       " 'mill',\n",
       " 'ei',\n",
       " 'naturally',\n",
       " 'stans',\n",
       " 'whereafter',\n",
       " 'wor',\n",
       " 'feel',\n",
       " 'ut',\n",
       " 'hehe',\n",
       " 'describe',\n",
       " 'frequently',\n",
       " 'white',\n",
       " 'yooooo',\n",
       " 'stupid',\n",
       " 'ea',\n",
       " 'whi',\n",
       " 'whereas',\n",
       " 'whom',\n",
       " 'hehehe',\n",
       " 'rm',\n",
       " 'at',\n",
       " 'on',\n",
       " 'overly',\n",
       " 'fam',\n",
       " 'loudly',\n",
       " 'each',\n",
       " 'extremely',\n",
       " 'yknow',\n",
       " 'yang',\n",
       " 'heart',\n",
       " 'hand',\n",
       " 'manifest',\n",
       " 'mau',\n",
       " 'tangina',\n",
       " 'okayy',\n",
       " 'become',\n",
       " 'yet',\n",
       " 'awww',\n",
       " 'seem',\n",
       " 'return',\n",
       " 'all',\n",
       " 'hate',\n",
       " 'naman',\n",
       " 'heck',\n",
       " 'suck',\n",
       " 'bitch',\n",
       " 'ahhhhh',\n",
       " 'lf',\n",
       " 'tak',\n",
       " 'or',\n",
       " 'blue',\n",
       " 'lmfaooooo',\n",
       " 'ooooo',\n",
       " 'es',\n",
       " 'toward',\n",
       " 'moot',\n",
       " 'complete',\n",
       " 'more',\n",
       " 'once',\n",
       " 'ahahaha',\n",
       " 'nothing',\n",
       " 'babe',\n",
       " 'pla',\n",
       " 'dead',\n",
       " 'uuuu',\n",
       " 'stan',\n",
       " 'another',\n",
       " 'gt',\n",
       " 'did',\n",
       " 'gb',\n",
       " 'down',\n",
       " 'next',\n",
       " 'sadly',\n",
       " 'gn',\n",
       " 'afterwards',\n",
       " 'ke',\n",
       " 'so',\n",
       " 'thank',\n",
       " 'aaa',\n",
       " 'welp',\n",
       " 'has',\n",
       " 'the',\n",
       " 'directly',\n",
       " 'ii',\n",
       " 'automatically',\n",
       " 'toooo',\n",
       " 'periodt',\n",
       " 'lmaoooo',\n",
       " 'could',\n",
       " 'publicly',\n",
       " 'aww',\n",
       " 'ao',\n",
       " 'omfg',\n",
       " 'obv',\n",
       " 'hahahahahah',\n",
       " 'nigga',\n",
       " 'unless',\n",
       " 'sometime',\n",
       " 'gu',\n",
       " 'mutuals',\n",
       " 'nose',\n",
       " 'congrats',\n",
       " 'uni',\n",
       " 'except',\n",
       " 'kak',\n",
       " 'lagi',\n",
       " 'purple',\n",
       " 'across',\n",
       " 'goooooo',\n",
       " 'out',\n",
       " 'yall',\n",
       " 'without',\n",
       " 'pero',\n",
       " 'few',\n",
       " 'ohhhhh',\n",
       " 'wonderful',\n",
       " 'would',\n",
       " 'hindi',\n",
       " 'literally',\n",
       " 'possibly',\n",
       " 'acc',\n",
       " 'whither',\n",
       " 'pogi',\n",
       " 'something',\n",
       " 'bec',\n",
       " 'whose',\n",
       " 'nearly',\n",
       " 'shi',\n",
       " 'sama',\n",
       " 'small',\n",
       " 'nak',\n",
       " 'every',\n",
       " 'beside',\n",
       " 'obviously',\n",
       " 'huh',\n",
       " 'ate',\n",
       " 'below',\n",
       " 'four',\n",
       " 'naurrr',\n",
       " 'meanwhile',\n",
       " 'today',\n",
       " 'mar',\n",
       " 'permanently',\n",
       " 'however',\n",
       " 'huhuhu',\n",
       " 'although',\n",
       " 'onto',\n",
       " 'hiiiii',\n",
       " 'honey',\n",
       " 'inside',\n",
       " 'its',\n",
       " 'oops',\n",
       " 'gp',\n",
       " 'until',\n",
       " 'thereafter',\n",
       " 'always',\n",
       " 'half',\n",
       " 'girlie',\n",
       " 'bu',\n",
       " 'never',\n",
       " 'therefore',\n",
       " 'thin',\n",
       " 'wat',\n",
       " 'some',\n",
       " 'tw',\n",
       " 'whew',\n",
       " 'tama',\n",
       " 'any',\n",
       " 'angry',\n",
       " 'ada',\n",
       " 'ooh',\n",
       " 'oj',\n",
       " 'happily',\n",
       " 'far',\n",
       " 'ourselves',\n",
       " 'differently',\n",
       " 'oh',\n",
       " 'then',\n",
       " 'ps',\n",
       " 'tyty',\n",
       " 'monday',\n",
       " 'couldnt',\n",
       " 'unfollow',\n",
       " 'cool',\n",
       " 'near',\n",
       " 'sweetie',\n",
       " 'luckily',\n",
       " 'plssss',\n",
       " 'hair',\n",
       " 'great',\n",
       " 'fl',\n",
       " 'surprise',\n",
       " 'of',\n",
       " 'cr',\n",
       " 'wo',\n",
       " 'ci',\n",
       " 'seemed',\n",
       " 'om',\n",
       " 'yourselves',\n",
       " 'other',\n",
       " 'eg',\n",
       " 'amongst',\n",
       " 'dd',\n",
       " 'i',\n",
       " 'lmfao',\n",
       " 'meeeee',\n",
       " 'surprisingly',\n",
       " 'favs',\n",
       " 'love',\n",
       " 'somewhere',\n",
       " 'her',\n",
       " 'ye',\n",
       " 'tall',\n",
       " 'ch',\n",
       " 'with',\n",
       " 'comment',\n",
       " 'simply',\n",
       " 'sk',\n",
       " 'does',\n",
       " 'ac',\n",
       " 'ooooh',\n",
       " 'ish',\n",
       " 'hahahahah',\n",
       " 'gw',\n",
       " 'sweet',\n",
       " 'lmaoo',\n",
       " 'sg',\n",
       " 'uwu',\n",
       " 'amazing',\n",
       " 'use',\n",
       " 'apparently',\n",
       " 'uhh',\n",
       " 'surely',\n",
       " 'tyy',\n",
       " 'though',\n",
       " 'l',\n",
       " 'happy',\n",
       " 'direct',\n",
       " 'theres',\n",
       " 'beh',\n",
       " 'later',\n",
       " 'mb',\n",
       " 'hihi',\n",
       " 'computer',\n",
       " 'naur',\n",
       " 'hasnt',\n",
       " 'mutual',\n",
       " 'anybody',\n",
       " 'co',\n",
       " 'ie',\n",
       " 'little',\n",
       " 'against',\n",
       " 'practically',\n",
       " 'aaaaaa',\n",
       " 'everywhere',\n",
       " 'pm',\n",
       " 'se',\n",
       " 'helloo',\n",
       " 'kasi',\n",
       " 'liv',\n",
       " 'big',\n",
       " 'dat',\n",
       " 'found',\n",
       " 'anyone',\n",
       " 'your',\n",
       " 'udah',\n",
       " 'po',\n",
       " 'md',\n",
       " 'sp',\n",
       " 'ultimately',\n",
       " 'mo',\n",
       " 'dw',\n",
       " 'doesnt',\n",
       " 'de',\n",
       " 'pfp',\n",
       " 'rather',\n",
       " 'ml',\n",
       " 'frrr',\n",
       " 'listen',\n",
       " 'noo',\n",
       " 'ri',\n",
       " 'bottom',\n",
       " 'slayyy',\n",
       " 'want',\n",
       " 'hot',\n",
       " 'sh',\n",
       " 'fin',\n",
       " 'apa',\n",
       " 'che',\n",
       " 'rt',\n",
       " 'efficiently',\n",
       " 'ho',\n",
       " 'ya',\n",
       " 'desperately',\n",
       " 'frr',\n",
       " 'becomes',\n",
       " 'lh',\n",
       " 'own',\n",
       " 'ft',\n",
       " 'basically',\n",
       " 'entirely',\n",
       " 'heyy',\n",
       " 'nahhh',\n",
       " 'thick',\n",
       " 'since',\n",
       " 'see',\n",
       " 'make',\n",
       " 'evening',\n",
       " 'aa',\n",
       " 'top',\n",
       " 'just',\n",
       " 'purposely',\n",
       " 'aint',\n",
       " 'idc',\n",
       " 'originally',\n",
       " 'among',\n",
       " 'x',\n",
       " 'tn',\n",
       " 'tuesday',\n",
       " 'sa',\n",
       " 'bg',\n",
       " 'red',\n",
       " 'ja',\n",
       " 'tryna',\n",
       " 'nd',\n",
       " 'wooo',\n",
       " 'ta',\n",
       " 'bad',\n",
       " 'jr',\n",
       " 'lately',\n",
       " 'congratulation',\n",
       " 'ow',\n",
       " 'occasionally',\n",
       " 'becoming',\n",
       " 'bcs',\n",
       " 'friday',\n",
       " 'why',\n",
       " 'pt',\n",
       " 'within',\n",
       " 'short',\n",
       " 'only',\n",
       " 'hehehehe',\n",
       " 'completely',\n",
       " 'after',\n",
       " 'insane',\n",
       " 'whereby',\n",
       " 'aaaaa',\n",
       " 'ol',\n",
       " 'oc',\n",
       " 'arent',\n",
       " 'beyond',\n",
       " 'find',\n",
       " 'sana',\n",
       " 'tooooo',\n",
       " 'helpp',\n",
       " 'upon',\n",
       " 'middle',\n",
       " 'ki',\n",
       " 'noona',\n",
       " 'further',\n",
       " 'aq',\n",
       " 'banget',\n",
       " 'by',\n",
       " 'not',\n",
       " 'ahh',\n",
       " 'tonight',\n",
       " 'hmmmm',\n",
       " 'saturday',\n",
       " 'most',\n",
       " 'cuteeeee',\n",
       " 'done',\n",
       " 'del',\n",
       " 'to',\n",
       " 'brain',\n",
       " 'bless',\n",
       " 'oo',\n",
       " 'ai',\n",
       " 'exp',\n",
       " 'as',\n",
       " 'slightly',\n",
       " 'an',\n",
       " 'again',\n",
       " 'arm',\n",
       " 'whereupon',\n",
       " 'hc',\n",
       " 'green',\n",
       " 'forty',\n",
       " 'eye',\n",
       " 'than',\n",
       " 'nooo',\n",
       " 'orange',\n",
       " 'almost',\n",
       " 'whyy',\n",
       " 'less',\n",
       " 'lmfaoooo',\n",
       " 'put',\n",
       " 'secretly',\n",
       " 'either',\n",
       " 'wi',\n",
       " 'there',\n",
       " 'excited',\n",
       " 'wala',\n",
       " 'dozen',\n",
       " 'goodnight',\n",
       " 'part',\n",
       " 'mainly',\n",
       " 'him',\n",
       " 'io',\n",
       " 'forever',\n",
       " 'side',\n",
       " 'le',\n",
       " 'eight',\n",
       " 'she',\n",
       " 'yk',\n",
       " 'ilysm',\n",
       " 'un',\n",
       " 'kst',\n",
       " 'helloooo',\n",
       " 'coz',\n",
       " 'cl',\n",
       " 'dm',\n",
       " 'asf',\n",
       " 'anywhere',\n",
       " 'along',\n",
       " 'low',\n",
       " 'least',\n",
       " 'my',\n",
       " 'lu',\n",
       " 'same',\n",
       " 'kg',\n",
       " 'significantly',\n",
       " 'therein',\n",
       " 'cmon',\n",
       " 'various',\n",
       " 'omggg',\n",
       " 'yesterday',\n",
       " 'thankfully',\n",
       " 'mga',\n",
       " 'reply',\n",
       " 'go',\n",
       " 'anyway',\n",
       " 'val',\n",
       " 'huhu',\n",
       " 'ohmygod',\n",
       " 'where',\n",
       " 'ours',\n",
       " 'must',\n",
       " 'tryy',\n",
       " 'need',\n",
       " 'else',\n",
       " 'lmfaoo',\n",
       " 'generally',\n",
       " 'nooooo',\n",
       " 'aha',\n",
       " 'both',\n",
       " 'pa',\n",
       " 'used',\n",
       " 'oomf',\n",
       " 'amoungst',\n",
       " 'noooo',\n",
       " 'their',\n",
       " 'ref',\n",
       " 'who',\n",
       " 'weirdly',\n",
       " 'huge',\n",
       " 'particularly',\n",
       " 'que',\n",
       " 'eleven',\n",
       " 'g',\n",
       " 'this',\n",
       " 'somehow',\n",
       " 'already',\n",
       " 'gose',\n",
       " 'bb',\n",
       " 'towards',\n",
       " 'ure',\n",
       " 'excuse',\n",
       " 'repeatedly',\n",
       " 'know',\n",
       " 'hereupon',\n",
       " 'nya',\n",
       " 'sixty',\n",
       " 'rarely',\n",
       " 'ge',\n",
       " 'none',\n",
       " 'ano',\n",
       " 'di',\n",
       " 'myself',\n",
       " 'aye',\n",
       " 'truly',\n",
       " 'due',\n",
       " 'should',\n",
       " 'yassss',\n",
       " 'gl',\n",
       " 'etc',\n",
       " 'ent',\n",
       " 'try',\n",
       " 'yah',\n",
       " 'ini',\n",
       " 'lmaoooooo',\n",
       " 'slay',\n",
       " 'vi',\n",
       " 'sorry',\n",
       " 'first',\n",
       " 'mm',\n",
       " 'somewhat',\n",
       " 'those',\n",
       " 'have',\n",
       " 'omgg',\n",
       " 'r',\n",
       " 'ms',\n",
       " 'these',\n",
       " 'vc',\n",
       " 'goodmorning',\n",
       " 'absolutely',\n",
       " 'unbelievably',\n",
       " 'body',\n",
       " 'ga',\n",
       " 'v',\n",
       " 'beforehand',\n",
       " 'didn',\n",
       " 'you',\n",
       " 'woooo',\n",
       " 'don',\n",
       " 'lik',\n",
       " 'yt',\n",
       " 'oooo',\n",
       " 'very',\n",
       " 'often',\n",
       " ...}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load custom stopwords\n",
    "with open(\"custom_stopwords.json\",\"r\") as f:\n",
    "    custom_stopwords=json.load(f)\n",
    "stop_words.update(custom_stopwords)\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3cc926f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of stopwords\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b958dc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902f1da3a4854a5c989324d654efe4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "def analyze_data_p2(TYPE):\n",
    "    with open(f\"Data/cleaned_data/p1/{TYPE}_cleaned.pkl\",'rb') as f:\n",
    "        data=pickle.load(f)\n",
    "    data.remove_stopwords(stop_words)\n",
    "    data.concatenate_post()\n",
    "    with open(f\"Data/cleaned_data/{TYPE}_cleaned.pkl\",\"wb\") as f:\n",
    "        pickle.dump(data,f)\n",
    "        \n",
    "for T in tqdm(MBTI_types):\n",
    "    analyze_data_p2(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a99c4",
   "metadata": {},
   "source": [
    "#### Demonstration of each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0af6be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "infp=Data_to_Analyze('infp',source=raw_data)\n",
    "infp.remove_url()\n",
    "infp.remove_mention_and_tag()\n",
    "infp.remove_emoji()\n",
    "infp.remove_whitespace()\n",
    "infp.remove_punct()\n",
    "test=infp.data.loc[0,\"posts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ece191ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3314"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans=0\n",
    "for sentence in test:\n",
    "    ans+=len(sentence.split(\" \"))\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d811e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ Thanks Forest ,  Thank you ,  Thanks ,  Than...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ Submission was made with the name The Matchs...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the only acceptable minion meme ,  WOW UR SO ...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[NO FUCKING WAY , SCREMSING AND CRYING ,  SAYR...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[when ur mom says u lost weight , i am insane ...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               posts  type\n",
       "0  [ Thanks Forest ,  Thank you ,  Thanks ,  Than...  infp\n",
       "1  [ Submission was made with the name The Matchs...  infp\n",
       "2  [the only acceptable minion meme ,  WOW UR SO ...  infp\n",
       "3  [NO FUCKING WAY , SCREMSING AND CRYING ,  SAYR...  infp\n",
       "4  [when ur mom says u lost weight , i am insane ...  infp"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infp.drop_non_english(0.75)\n",
    "infp.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04c4a268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Thanks Forest, Thank you, Thanks, Thank you E...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Submission was made with the name The Matchst...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the only acceptable minion meme, WOW you are ...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[NO FUCKING WAY, SCREMSING AND CRYING, SAYR CU...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[when you are  mom says you  lost weight, i am...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               posts  type\n",
       "0  [Thanks Forest, Thank you, Thanks, Thank you E...  infp\n",
       "1  [Submission was made with the name The Matchst...  infp\n",
       "2  [the only acceptable minion meme, WOW you are ...  infp\n",
       "3  [NO FUCKING WAY, SCREMSING AND CRYING, SAYR CU...  infp\n",
       "4  [when you are  mom says you  lost weight, i am...  infp"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infp.expand_contractions()\n",
    "infp.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43d6fd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[thanks forest, thank you, thanks, thank you e...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[submission was made with the name the matchst...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the only acceptable minion meme, wow you are ...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[no fucking way, scremsing and crying, sayr cu...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[when you are  mom says you  lost weight, i am...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               posts  type\n",
       "0  [thanks forest, thank you, thanks, thank you e...  infp\n",
       "1  [submission was made with the name the matchst...  infp\n",
       "2  [the only acceptable minion meme, wow you are ...  infp\n",
       "3  [no fucking way, scremsing and crying, sayr cu...  infp\n",
       "4  [when you are  mom says you  lost weight, i am...  infp"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infp.tolower()\n",
    "infp.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36bf5e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[thanks forest, thank you, thanks, thank you e...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[submission was made with the name the matchst...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the only acceptable minion meme, wow you are ...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[no fucking way, scremsing and crying, sayr cu...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[when you are  mom says you  lost weight, i am...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               posts  type\n",
       "0  [thanks forest, thank you, thanks, thank you e...  infp\n",
       "1  [submission was made with the name the matchst...  infp\n",
       "2  [the only acceptable minion meme, wow you are ...  infp\n",
       "3  [no fucking way, scremsing and crying, sayr cu...  infp\n",
       "4  [when you are  mom says you  lost weight, i am...  infp"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infp.remove_punct()\n",
    "infp.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db778622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[thanks, forest], [thank, you], [thanks], [th...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[submission, was, made, with, the, name, the,...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[the, only, acceptable, minion, meme], [wow, ...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[no, fucking, way], [scremsing, and, crying],...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[when, you, are, mom, says, you, lost, weight...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               posts  type\n",
       "0  [[thanks, forest], [thank, you], [thanks], [th...  infp\n",
       "1  [[submission, was, made, with, the, name, the,...  infp\n",
       "2  [[the, only, acceptable, minion, meme], [wow, ...  infp\n",
       "3  [[no, fucking, way], [scremsing, and, crying],...  infp\n",
       "4  [[when, you, are, mom, says, you, lost, weight...  infp"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infp.totokens()\n",
    "infp.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2055354f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[thanks, forest], [thank, you], [thanks], [th...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[submission, be, make, with, the, name, the, ...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[the, only, acceptable, minion, meme], [wow, ...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[no, fucking, way], [scremsing, and, cry], [s...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[when, you, be, mom, say, you, lose, weight],...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               posts  type\n",
       "0  [[thanks, forest], [thank, you], [thanks], [th...  infp\n",
       "1  [[submission, be, make, with, the, name, the, ...  infp\n",
       "2  [[the, only, acceptable, minion, meme], [wow, ...  infp\n",
       "3  [[no, fucking, way], [scremsing, and, cry], [s...  infp\n",
       "4  [[when, you, be, mom, say, you, lose, weight],...  infp"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infp.post_lemmatize()\n",
    "infp.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a5beb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[forest], [], [], [elizabeth], [holly], [], [...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[submission, matchstickguy], [real], [], [pre...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[acceptable, minion, meme], [wow, xiao, real]...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[fucking, way], [scremsing], [sayr], [aaaaaaa...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[mom, lose, weight], [], [worth, honest], [up...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               posts  type\n",
       "0  [[forest], [], [], [elizabeth], [holly], [], [...  infp\n",
       "1  [[submission, matchstickguy], [real], [], [pre...  infp\n",
       "2  [[acceptable, minion, meme], [wow, xiao, real]...  infp\n",
       "3  [[fucking, way], [scremsing], [sayr], [aaaaaaa...  infp\n",
       "4  [[mom, lose, weight], [], [worth, honest], [up...  infp"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infp.remove_stopwords(stop_words)\n",
    "infp.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b2fff1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[forest, elizabeth, holly, fuss, fly, seat, pa...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[submission, matchstickguy, real, premier, vic...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[acceptable, minion, meme, wow, xiao, real, fr...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[fucking, way, scremsing, sayr, aaaaaaaaaaaa, ...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[mom, lose, weight, worth, honest, update, str...</td>\n",
       "      <td>infp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               posts  type\n",
       "0  [forest, elizabeth, holly, fuss, fly, seat, pa...  infp\n",
       "1  [submission, matchstickguy, real, premier, vic...  infp\n",
       "2  [acceptable, minion, meme, wow, xiao, real, fr...  infp\n",
       "3  [fucking, way, scremsing, sayr, aaaaaaaaaaaa, ...  infp\n",
       "4  [mom, lose, weight, worth, honest, update, str...  infp"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infp.concatenate_post()\n",
    "infp.data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
