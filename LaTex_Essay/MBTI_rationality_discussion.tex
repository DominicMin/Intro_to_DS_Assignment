%============================================================
%  Xiamen University Malaysia Article Template
%  Compile with XeLaTeX or LuaLaTeX to ensure Times New Roman
%============================================================
\documentclass[12pt]{article}

%------------------------------------------------------------
% Packages
%------------------------------------------------------------
\usepackage{fontspec}      % For Times New Roman
\usepackage{setspace}      % For line spacing
\usepackage{geometry}      % Page margins
\usepackage{fancyhdr}      % Header & footer
\usepackage{titlesec}      % Custom headings
\usepackage{csquotes}      % Quotation tools (optional)
\usepackage{listings}
\usepackage{xcolor} 
\usepackage{float}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue,citecolor=black]{hyperref}
\usepackage[backend=biber,style=apa]{biblatex}
\addbibresource{references.bib}  % 不要加 .bib 扩展名


%------------------------------------------------------------
% Fonts & Spacing
%------------------------------------------------------------
\setmainfont{Times New Roman}
\setstretch{1.5}           % 1.5 line spacing

%------------------------------------------------------------
% Geometry (1‑inch margins by default)
%------------------------------------------------------------
\geometry{a4paper, margin=1in}

%代码格式
\lstnewenvironment{python}[1][]
{
	\lstset{
		language=Python,
		basicstyle=\ttfamily\tiny,
		numbers=left,
		numberstyle=\tiny\color{gray},
		numbersep=8pt,
		frame=none,
		framesep=2pt,
		xleftmargin=0pt,
		framexleftmargin=2pt,
		backgroundcolor=\color[RGB]{245,245,245},
		breaklines=true,
		breakindent=10pt,
		keywordstyle=\color[RGB]{255,119,0},
		morekeywords={as, self},
		deletekeywords={print},
		keywordstyle=[2]\color[RGB]{144,0,144},
		morekeywords=[2]{print},
		stringstyle=\color[RGB]{0,170,0},
		commentstyle=\color[RGB]{221,0,0},
		showstringspaces=false,
		#1
	}
}{}


%------------------------------------------------------------
% Header & Footer
%------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}                              % Clear default header/footer
\fancyhead[C]{XIAMEN UNIVERSITY MALAYSIA} % Centered header text
\fancyfoot[C]{\thepage}                % Centered page number
\renewcommand{\headrulewidth}{0pt}      % Remove header line

%------------------------------------------------------------
% Title & Section Formatting (14‑pt Times New Roman, bold)
%------------------------------------------------------------
\titleformat{\section}
{\fontsize{14pt}{16pt}\selectfont\bfseries}
{\thesection}{1em}{}

\titleformat{\subsection}
{\fontsize{14pt}{16pt}\selectfont\bfseries}
{\thesubsection}{1em}{}

% Optional: adjust subsubsection as “subtitle” too
\titleformat{\subsubsection}
{\fontsize{14pt}{16pt}\selectfont\bfseries}
{\thesubsubsection}{1em}{}

% Ensure normal text stays 12‑pt Times New Roman (set by documentclass)

%------------------------------------------------------------
% Document begins
%------------------------------------------------------------
\begin{document}
	
	% ----------------- Main Content ----------------------------
	
	% Example Title (remove if not needed)
	%\section*{Sample Document Title}
	%\addcontentsline{toc}{section}{Sample Document Title}
	
	\section{Introduction}
	With the recent renewal of the MBTI's popularity, posts or analyses about it have become widespread on public platforms. It seems that MBTI has become a way people use at first meetings.
	
	The Myers-Briggs Type Indicator (MBTI) is a psychological framework primarily used to describe and measure personality traits by categorizing individuals into four dichotomous dimensions. In essence, it aims to reflect how people perceive the world and make decisions based on their internal experiences(\cite{yang2022research}).
	
	And the test is conducted through asking a series of questions, and the type of question is kind of like asking about the tendency to do something. Based on the responses, individuals receive a result represented by four letters, each corresponding to one of the four bipolar dimensions. According to \textcite{boyle1995mbti}, these four dichotomous dimensions classify individuals as either extraverted (E) or introverted (I), sensing (S) or intuitive (N), thinking (T) or feeling (F), and judging (J) or perceiving (P). Combinations of the four preferences generate one of 16 personality types (e.g., ESFJ, ENFP, INTP, ISFJ), each associated with distinct behavioral tendencies, reflecting differences in attitudes, orientation, and decision-making styles. Also, on the official website, it will provide the future career options for you, like INFP, which may be more suitable for being an author. The percentage of each standard will also show in the final result.
	
	The MBTI provides a widely recognized and accessible way to understand personality, making it a useful foundation for further behavioral and data-driven analysis.
	
	\section{Objective}
	\begin{enumerate} 
		\item Evaluate the reliability of the MBTI from a statistical perspective.    \item Explore the potential application of MBTI in social media behavior analysis.    
		\item Help people better understand personality traits and behavioral patterns.    
		\item Helping people eliminate stereotypes caused by MBTI personality types.
	\end{enumerate}
	
	\section{Problem Statements}
	\begin{enumerate}
		\item Are the results of the MBTI personality test statistically robust and reliable?
		\item Do the four dimensions of MBTI work independently, or are they connected in some way?
		\item Do people with different MBTI types have different levels of activity on the internet?
		\item Do significant differences exist in the interest preferences and behavioral patterns of different MBTI personality types on social networking sites?
	\end{enumerate}
	
	\section{Data Collection}
	\begin{enumerate}
		\item “MBTI Personality Type Twitter Dataset”
		
		Tweets were originally harvested from the public Twitter API by a third-party collector and later released on Kaggle by Mazlumi(\url{https://www.kaggle.com/datasets/mazlumi/mbti-personality-type-twitter-dataset})
		\begin{itemize}
			\item 8,600 Twitter users, and 1 million raw tweets.
			\item Each user record contains the self-declared MBTI type (e.g., “ENFP”) taken from the user’s bio.
			\item The text has not been further cleaned or filtered—URLs, emojis, hashtags, and retweets remain. Researchers must therefore perform their preprocessing (tokenisation, stop-word removal, emoji handling, etc.) before analysis.    
		\end{itemize}
		\item “KPMIRU Questionnaires Data”
		
		Questionnaire responses compiled and shared on Kaggle by Pmenshih(\url{https://www.kaggle.com/datasets/pmenshih/kpmiru-questionnaires-data})
		\begin{itemize}
			\item Contains every participant’s item-level answers to the full KPMIRU personality inventory (several dozen Likert-scale questions).
			\item Provides the scored results for all four MBTI dimensions—reported as continuous scores (0–100 per axis) as well as the final type label (e.g., “ISTJ”).
			\item Demographic fields (age range, gender, education) are included, enabling richer statistical controls. 
		\end{itemize}
		
		Together, the Twitter dataset supplies large-scale, real-world language samples with self-reported types, while the KPMIRU dataset offers clean, psychometrically scored questionnaire data. The two sources complement each other for training and validating our emotion-aware MBTI models.
	\end{enumerate}
	\begin{figure}[H]
		\centering
		\includegraphics{Q1P1} %这个图片怎么这么糊啊(┬┬﹏┬┬),到时候让csq重新截图给我
		\caption{reading data sets}
	
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{Q1P2}
		
	\end{figure}
	As illustrated in the two Figures above, the combined Kaggle sources provide information on:
	\begin{enumerate}
		\item Self-reported MBTI types for each respondent
		\item Raw Twitter posts and basic tweet metadata linked to those MBTI labels  
		\item Demographic and psychometric questionnaire answers (KPMIRU survey) 
		\item Behavioral metrics such as posting frequency and topic keywords extracted from the tweets
	\end{enumerate}
	\section{MBTI in Measurement} %改标题名字
	\subsection{Data Cleaning and Pre-processing}
	\subsubsection{Handling Missing Values}
	\begin{enumerate}
		\item We use dropna() to eliminate any records with missing or null entries. Fortunately, the dataset had no missing psychotype labels or scoring data.
			\begin{figure}[H]
			\centering
			\includegraphics[width=0.9\textwidth]{Q1P4}
			
		\end{figure}
	\end{enumerate}
	\subsubsection{Outlier Detection and Correction}
	\begin{enumerate}
		\item Outliers in numeric scores were identified using the IQR (interquartile range) method.
		\item For each numeric column, we computed Q1, Q3. IQR = Q3 - Q1 represent the middle 50\% of the data distribution. The lower\_bound and upper\_bound represent the boundaries of the normal range under the Interquartile Range (IQR) rule. And then we replaced values outside [Q1 − 1.5×IQR, Q3 + 1.5×IQR] with the column median. 
		\item The median substitution method mainly serves to stabilize the overall distribution and avoid the influence of extreme values.
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.9\textwidth]{Q1P5}
			
		\end{figure}
	\end{enumerate}
	\subsubsection{Export the cleaned data}
	After completing the outlier replacement and data cleaning process, we used df.info() to verify the integrity and structure of the cleaned dataset. The cleaned DataFrame was then exported using the to\_csv() method, which saved it as kpmi\_ru\_data(Cleaned).csv for downstream analysis. The index=False parameter ensured that row indices were not written to the CSV file.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{Q1P6}
		
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{Q1P7}
		
	\end{figure}
	The data in the figure 3.1 is the data after our data cleaning.
	
	\subsection{Model Building and Evaluation}
	Chi-square test is an on parametric statistical test to determine if the two or more classifications of the samples are independent or not\cite{zibran2007chi}. We all know that MBTI has four dimensions(Extraversion–Introversion (E–I), Sensing–Intuition (S–N), Thinking–Feeling (T–F) and Judging–Perceiving (J–P)). But whether or not these four dimensions interrelated or independent of each other stays unknown. For explanation, let’s consider the data presented in Figure 3.1 which comprising 100 000 respondents, providing information on the scoring fields of the four dimensions of MBTI. In order to find the answer, we use the chi-square test.
	
	Each respondent’s type label (e.g., ENTP) was decomposed into its four constituent letters, and the sample was cross-classified into a 2 × 2 × 2 × 2 contingency table (16 cells)(shown in figure 3.2) 
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{Q1P8}
		
	\end{figure}
	Then we calculate the marginal distribution (the distribution of each dimension separately), for example, N\_IE = [count(E), count(I)](shown in figure3.3)
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{Q1P9}
		
	\end{figure}
	Under the null hypothesis of mutual independence, the expected frequency in each cell was computed as the product of the four one-dimensional marginal distributions multiplied by the sample size.(shown in figure3.4)
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{Q1P10}
		
	\end{figure}
	Now, it’s time to calculate the Pearson chi-square statistic, while O is observed value and E is expected value.(shown in figure 3.5)
	\[
	\chi^2 = \sum \frac{(O - E)^2}{E}
	\]
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{Q1P10}
		
	\end{figure}
	Finally, with χ² = 1342.93 and df = 11,we can compute the right - tailed probability corresponding to the chi - square statistic under 10 degrees of freedom, which is 0(shown in figure 3.6). Because the p-value falls far below the conventional α =0.05 threshold, the null hypothesis is decisively rejected: the observed joint distribution of MBTI preferences deviates dramatically from what would be expected if the four indices varied independently. In practical terms, substantial associations exist among the E–I, S–N, T–F and J–P scales, corroborating earlier psychometric critiques that the MBTI dimensions are not orthogonal factors but overlap to a non-trivial extent. Actually, some researchers had also proven that the four dimensions of MBTI are not independent. \textcite{fleenor1997relationship} investigated the intercorrelations among MBTI continuous scores and found that while most dimension pairs demonstrated relatively low correlations, the correlation between the Sensing–Intuition (SN) and Judging–Perceiving (JP) scales was notably higher. Specifically, the study reported a correlation coefficient of r = 0.41 between SN and JP, indicating a moderate positive relationship. This finding has been replicated in other research and suggests that individuals who prefer intuition are more likely to also prefer perceiving. As such, the assumption of strict statistical independence between MBTI preference axes, particularly between SN and JP, may not hold, which is in line with the findings of our study.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{Q1P11}
		
	\end{figure}
	
	\subsection{Exploratory Data Analysis (EDA)}
	\subsubsection{MBTI Type Frequency}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{Q1P3EDA1}
		
	\end{figure}
	
	A frequency bar chart of the 16 MBTI types was generated using the value\_counts() function on the psychotype column. The result was plotted using matplotlib and is shown in Figure 4.1.1.
	%后面需要统一图片命名！！
	\begin{figure}[H]
		\centering
		\includegraphics{Q1EDA2} 
		\caption{Frequency of 16 MBTI Types}		
	\end{figure}
	
	As seen in Figure 4.1.1 above, the frequency of 16 MBTI types has been shown in a descending order. The type ESTJ ranks first, which shows that, based on our dataset, the proportion of ESTJ people is the highest. Conversely, the proportion of INFP people is the lowest. Additionally, it can be observed that the \_S\_J people rankings are relatively high in frequency, as the \_NF\_ people rankings are relatively low.
	
	Also, Figure 4.1.1 exhibits severe right oblique imbalance distribution, as the number of ESTJ people is nearly nine times as many as the number of INFP people. This phenomenon may be attributed to our database being originally from Kaggle, and certain MBTI type tends to participate in such investigations.
	
	This contrasts with national MBTI distribution statistics reported in the MBTI® Manual, where ISFJ and ESFJ were found to be the most common types among U.S. adults (Myers et al., 1998), suggesting that our dataset, to a certain extent, fits the population-level trends. In distinct regions, the regional differences may influence MBTI type distributions in specific rankings.
	
	\subsubsection{Distribution of MBTI Dimension Scores}
		\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{Q1EDA3}
		
	\end{figure}
	
	Histograms of scores in each MBTI dimension were generated using pandas.DataFrame.hist and visualized with matplotlib as shown in Figure 4.1.2.
	\begin{figure}[H]
		\centering
		\includegraphics{Q1EDA4} 
		\caption{Histogram of MBTI Dimension Scores}		
	\end{figure}
	
	As seen in Figure 4.1.2 above, from the score concentration trend, the eight MBTI poles vary in concentration. For example, E, S, and T scores are generally spread from 10 to 40, while N and I scores are more narrowly concentrated between 5 to 30. And in detail, in the dimension of P and T, exhibit higher scores, which shows the preference toward Thinking and Perceiving. Compared to them, in the dimension of N and I, they exhibit lower scores, which indicates most people are more inclined toward Extraversion and Sensing.
	
	From the data obtained from these diagrams, we can observe that most samples in these dimensions show distributional asymmetries, as not evenly distributed. Particularly for the high values of J and low values of N, this result matches the observation at 4.1.1 that \_S\_J types dominate the database.
	
	\subsubsection{Descriptive Statistics of MBTI Dimensions}
	\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{Q1EDA5}
	
	\end{figure}	
	
	Descriptive statistics of each MBTI dimension were computed and consolidated into a single summary table, including skewness and kurtosis, to facilitate score distribution analysis shown in Table 4.1.1
	%这里table要改！！
	\begin{figure}[H]
		\centering
		\includegraphics{Q1EDA6} 
		\caption{Descriptive Statistics of MBTI Dimension Scores}		
	\end{figure}
	
	%还有交叉引用要改（不过是不是不改也行？	
	Based on Table 4.1.3, it can be observed that the dimensions of I, S, N, and F showcase a notable positive skew distribution, as the dimension of N, with a value of -0.11, demonstrates left skewness most. This phenomenon tends to show that most people are more likely to be Sensing(S) and so on. And in contrast, the dimension of P with the value of +0.05 exhibits right skewness, implying a preference towards lower values, showing that Judging(J) dominates more in this database. Results of these also support the conclusion in Figure 4.1.1 that \_S\_J people occupy the majority.
	
	And from the kurtosis value, it can be seen that most values are around 0 and negative, which suggests that all the dimension is distributed platykurtic. With relatively concentrated values, this exhibits the loss of extreme outliers. Also, the maximum value and minimum value that all dimensions have can be used in the form of Max-Min, which demonstrates that J and P dimensions have the largest difference, indicating that individual differences between them are the strongest.
	\begin{figure}[H]
		\centering
		\includegraphics{Q1EDA7} 
		\caption{MBTI Distribution of 16 Personality Types}		
	\end{figure}
	
	Figure 4.1.3 MBTI Distribution of 16 Personality Types from Jang and Kim (2014), supporting the dominance of \_S\_J types observed in our dataset. 
	
	\subsubsection{Correlation Analysis Between MBTI Dimensions}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{Q1EDA8}
		
	\end{figure}	
	A Pearson correlation heatmap was constructed to visualize linear relationships among MBTI dimension scores, as shown in Figure 4.1.4.
	\begin{figure}[H]
		\centering
		\includegraphics{Q1EDA9} 
		\caption{Correlation Heatmap of MBTI Dimension Scores}		
	\end{figure}
	
	As seen in Figure 4.1.4 above, the correlation between the opposite dimensions, like E vs I, S vs N, T vs F, and J vs P. All of them are strongly negative, which is consistent with the design concept of MBTI, as the two poles of each dimension are opposed. As a result, the MBTI method has its rationality. This result aligns with (Li et al., 2024), who also observed that “the correlations between the four axes are generally weak, indicating that the personality traits on each axis are relatively independent” (p. 12).
	
	Furthermore, in Figure 4.1.4, the correlation between other dimension pairs is low are nearly 0. This phenomenon supports that the dimensions are seemingly independent. While the correlation between S and T is +0.16, a bit larger than others, it indicates that someone who is Sensing(S) tends to be Thinking(T). In contrast, the correlation between S and P is -0.13, which indicates that those who are Sensing(S) tend to be Judging(J).
	%先综述主题建模是什么——》数据清洗步骤必要性解释，结果展示-》主题建模步骤-》EDA
	%intro需要解释objective的原因，将文章分为两个分析方向，解释两者之间的联系；MBTI的进一步解释有必要，写什么根据我后面EDA决定
	\section{MBTI in Manifestation} 
	In the previous section, we examined the statistical characteristics of MBTI. In this section, we turn to the manifestation level by investigating how different MBTI types vary in their topic preferences and expressive patterns on Twitter.	
	
	
	\subsection{Topic Preferences of MBTI Personalities}
	To analyze the topic preference of different MBTI personalities, we adopt LDA modeling.
	
	Latent Dirichlet Allocation(LDA) is a generative probabilistic model for uncovering hidden “topics” in a large collection of documents. It assumes each document is composed of a mixture of topics, and each topic is represented by a distribution over words. For example, a document might be 30\% about "technology" and 70\% about "health", with each topic associated with its own common vocabulary.
	
	In the LDA framework, the generation of a document is imagined as a two-step process:
	\begin{enumerate}
		\item a topic distribution is assigned to the document.
		\item For each word in the document:
		\begin{itemize}
			\item A topic is randomly chosen based on the document’s topic distribution.
			\item A word is selected from the vocabulary of that topic.
		\end{itemize}
	\end{enumerate}
	During model training, this generative process is reversed: LDA infers the underlying topic structure based on the observed words in the documents. It estimates:
	\begin{itemize}
		\item the topic proportions for each document
		\item the most representative words for each topic
	\end{itemize}
	
	The specific LDA process of our project will be discussed in detail. But before we delve into it, we need to make sure our dataset is clean and ready for LDA modeling. Because LDA relies on word‐frequency patterns to infer hidden topics, it’s important to remove noise, such as words that are meaningless or unrepresentative, so the model produces reliable topics.
	\subsubsection{Data Cleaning}
	\subsubsection{LDA modeling}
	The cleaned data are organized by MBTI types, each as a tokenized and preprocessed text collection stored in a structured DataFrame. To perform LDA modeling, the input must be converted into a Bag-of-Words (BoW) corpus, where each document is represented as a list of (word\_id, frequency) tuples. This requires mapping each unique word to a distinct integer ID, resulting in a dictionary that captures the vocabulary of the entire corpus. In this assignment, we apply filtering by removing words that appear in more than 20\% of documents (no\_above=0.2) and those that appear in fewer than 50 documents (no\_below=50) to retain only representative terms.
	\begin{python}
import gensim.corpora as corpora
from gensim.models import CoherenceModel
def constract_initial_dict(source,no_above,no_below):

    output = {T: {
        "original_text": [],
    } for T in MBTI_types}
    
    output["all_original_text"]=[]
    for T in tqdm(MBTI_types):
        for i in source[T].data.index:
            temp=source[T].data.loc[i,"posts"]
            output[T]["original_text"].append(temp)
        output["all_original_text"].extend(output[T]["original_text"])
    output["overall_dict"]=corpora.Dictionary(output["all_original_text"])
    output["overall_dict"].filter_extremes(no_above=no_above,no_below=no_below)
    output["overall_dict"].compactify()
    print("Size of dictionary:",len(output["overall_dict"]))
    output["all_corpus"]=[output["overall_dict"].doc2bow(post_token) for post_token in output["all_original_text"]]
    return output
initial_dict=constract_initial_dict(source=cleaned_data,
                                    no_above=0.20,
                                    no_below=50)
with open("Data/initial_dict.pkl",'wb') as f:
    pickle.dump(initial_dict,f)
	\end{python}
	
	For a convenient inspection, we computed the overall term‐frequency distribution by summing each token’s bag‐of‐words counts across the entire corpus and stored the result table as a CSV file.
	\begin{python}
def check_corpus(corpus,dict,name=''):
    result=pd.DataFrame(
        [
            list(range(len(dict))),
            [0]*len(dict)
        ]
    ).T
    result.columns=["word","frequency"]
    for post in tqdm(corpus):
        for word_tuple in post:
            result.loc[word_tuple[0],"frequency"]+=word_tuple[1]
    for i in result.index:
        result.loc[i,"word"]=dict[i]
    result=result.sort_values(by="frequency",ascending=False)
    result.to_csv(f"Data/{name}id2word_result.csv")
	\end{python}
	
	Selecting an appropriate number of topics is crucial for LDA modeling. When the topic number is too small, unrelated content may be merged into the same topic, reducing interpretability. Conversely, an excessively large number of topics may fragment coherent semantic themes, leading to redundancy and overlap among topics.To determine the optimal number of topics for the LDA model, we trained multiple models with different numbers of topics. The semantic coherence and interpretability of the resulting topics were evaluated using the c\_v coherence score, which measures how consistently related the top words within each topic are. 
	\begin{python}
with open("Data/initial_dict.pkl","rb") as f:
    initial_dict=pickle.load(f)
import gensim.corpora as corpora
import gensim
from gensim.models import LdaMulticore,CoherenceModel

def optimize_topic_num(
  start,
  end,
  step,
  dict=initial_dict["overall_dict"],
  corpus=initial_dict["all_corpus"],
  text=initial_dict["all_original_text"]      
):
  output=pd.Series({},dtype=float)
  topic_num_range=range(start, end+1, step)
  for topic_num in tqdm(topic_num_range, desc="Calculating optimal topic number"):
    # Train the LDA model (on all post data)
    temp_lda_model=LdaMulticore(
        corpus=corpus,    # Use the bag-of-words corpus of all posts
        id2word=dict,     # Use the global dictionary
        num_topics=topic_num,
        random_state=100,
        chunksize=100,    # Reduce chunksize to speed up update frequency
        passes=10,        # Reduce passes to shorten total training time
        iterations=50,    # Specify the number of iterations per pass
        alpha=0.01,       # Use a smaller fixed value to promote topic sparsity
        eta=0.01,         # Use a smaller fixed value to promote word sparsity
        per_word_topics=False,  
        workers=None                    
    )

    # Evaluate the model 
    temp_chmodel=CoherenceModel(
        model=temp_lda_model,
        texts=text,
        dictionary=dict,
        coherence="c_v"
    )
    output[topic_num]=temp_chmodel.get_coherence()
  print(output)
	\end{python}
	
	After a few trials, we find that 19 topics gives us the highest c\_v coherence score. 
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\textwidth]{Q2CV} 
		\caption{\centering Coherence scores for topic numbers 19 to 24. The highest score is observed at 19 topics, suggesting it as the optimal choice for this model.}		
	\end{figure}
	
	We train the final LDA model with 19 topics using optimized settings,evaluate the model with c\_v coherence score and save the model outputs, cleaned corpus, and topic descriptions for further analysis.
	\begin{python}
# Train the optimized final model with enhanced parameters for better convergence and topic separation
topics = 19

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# Optimized parameters for better convergence and topic distinction
lda_model = LdaMulticore(
    corpus=initial_dict["all_corpus"],
    id2word=initial_dict["overall_dict"],
    num_topics=topics,
    random_state=100,
    chunksize=100,           # Significantly reduced: increases update frequency and improves convergence
    passes=300,              # Moderately reduced: adjusted to work with other optimized parameters
    iterations=150,          # Newly added: increase iterations per pass
    alpha=0.01,              # Changed from 'asymmetric' to a small value: promotes document-topic sparsity and improves topic distinction
    eta=0.01,                # Changed from 'auto' to a small value: promotes word-topic sparsity and reduces topic mixing
    decay=0.5,               # Newly added: controls learning rate decay, improving convergence stability
    offset=1.0,              # Newly added: initial value for learning rate
    minimum_probability=0.01, # Newly added: filters out low-probability topic assignments
    per_word_topics=False,
    workers=None,            # Enables parallelization to speed up training
    eval_every=20            # Reduces evaluation frequency to lower computational cost
)
# Model evaluation
chmodel = CoherenceModel(
        model=lda_model,
        texts=initial_dict["all_original_text"],
        dictionary=initial_dict["overall_dict"],
        coherence="c_v"
    )
cv=chmodel.get_coherence()
cv
# Create unique directories for each LDA model
# That's because all variables are unique for each LDA model due to different stopword set
model_id=f"{topics}_{str(cv)[2:6]}"
# Model ID are designed as "[number of topics]_[CV score]", is unique for each model

path=f"output/lda_model/lda_{model_id}"
if not os.path.exists(path):
    os.makedirs(path)

path=f"output/lda_model/lda_{model_id}/cleaned_data"
if not os.path.exists(path):
    os.makedirs(path)

path=f"output/lda_model/lda_{model_id}/visualization"
if not os.path.exists(path):
    os.makedirs(path)

# Save LDA model
with open(f"output/lda_model/lda_{model_id}/lda_{model_id}.pkl",'wb') as f:
    pickle.dump(lda_model,f)

# Save cleaned data
with open(f"output/lda_model/lda_{model_id}/cleaned_data/cleaned_data.pkl", "wb") as f:
    pickle.dump(cleaned_data,f)

# Save original text
with open(f"output/lda_model/lda_{model_id}/all_original_text.pkl","wb") as f:
        pickle.dump(initial_dict["all_original_text"],f)
        
# Get all topics words and weights
all_topics_words = lda_model.show_topics(num_topics=-1, num_words=40, formatted=False)

markdown_content=f"## {topics} topics, cv={str(cv)[2:6]}\n\n"

for topic_id, topic_words_with_weights in all_topics_words:
        markdown_content += f"### Topic {topic_id}:\n" 
        
        
        for word, weight in topic_words_with_weights:
            markdown_content += f"- `{word}`: {weight:.4f}\n"
        markdown_content += "\n" 

# Save topic words and weights to markdown file
with open(f"output/lda_model/lda_{model_id}/lda_{model_id}.md", "w", encoding="utf-8") as f:
        f.write(markdown_content)
	\end{python}
	
	After examining the model outputs, we observed that some meaningless or unrepresentative words still remain in the model. This is primarily because the Twitter dataset contains a substantial amount of slang that was not fully removed during the initial data cleaning process. To address this issue, we manually add these terms to the stopword list and re-run the LDA modeling process iteratively until we obtain a model with a high c\_v coherence score and minimal noise from irrelevant terms.
	
	The model we finally settled on is model 19\_5687, with the highest high c\_v coherence score 0.5687. With this model, we will visualize the features of MBTI Twitter topics.
	
	
	\subsubsection{Result Visualization and Topic Evaluation}
	To start up the topic evaluation process, we need to import necessary libraries; define MBTI types,dimensions and groups; as well as load LDA model and cleaned data.
		\begin{python}
# Import necessary libraries
%matplotlib widget
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
import seaborn as sns
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import pandas as pd
import numpy as np
import pickle
from tqdm.auto import tqdm
import gensim.corpora as corpora
from gensim.models import LdaModel
from collections import defaultdict
from data_clean import Data_to_Clean,Data_to_Analyze
import warnings
warnings.filterwarnings('ignore')
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Ensure the plots are displayed correctly
plt.rcParams['axes.unicode_minus']=False
sns.set_style("whitegrid")
sns.set_palette("husl")

# Define MBTI types
MBTI_types=[
    'istj','isfj','infj','intj',
    'istp','isfp','infp','intp',
    'estp','esfp','enfp','entp',
    'estj','esfj','enfj','entj'
]

# Define MBTI dimensions
mbti_dimensions={
    'E': ['estp','esfp','enfp','entp','estj','esfj','enfj','entj'],
    'I': ['istj','isfj','infj','intj','istp','isfp','infp','intp'],
    'S': ['istj','isfj','istp','isfp','estp','esfp','estj','esfj'],
    'N': ['infj','intj','infp','intp','enfp','entp','enfj','entj'],
    'T': ['intj','intp','entj','entp','istj','istp','estj','estp'],
    'F': ['isfj','infj','isfp','infp','esfj','enfj','esfp','enfp'],
    'J': ['istj','isfj','infj','intj','estj','esfj','enfj','entj'],
    'P': ['istp','isfp','infp','intp','estp','esfp','enfp','entp']
}

# Define MBTI groups
mbti_groups={
    "analysts":["intj","intp","entj","entp"],
    "diplomats":["infj","infp","enfj","enfp"],
    "sentinels":["istj","isfj","istp","isfp"],
    "explorers":["isfp","istp","estp","esfp"]
}

# Load LDA model and data
# Model ID are designed as "[number of topics]_[CV score]",is unique for each model


def load_lda_data():
    # Load LDA model
    lda_model=pickle.load(open(f"output/lda_model/lda_{model_id}/lda_{model_id}.pkl","rb"))
    print(f"Successfully loaded LDA model with {lda_model.num_topics} topics")
    
    # Load original text data
    all_original_text=pickle.load(open(f"output/lda_model/lda_{model_id}/all_original_text.pkl","rb"))
    print(f"Successfully loaded original text data with {len(all_original_text)} documents")
    
    return lda_model,all_original_text
    

# Load cleaned data grouped by MBTI types
def load_mbti_data():
    file_path=f"output/lda_model/lda_{model_id}/cleaned_data/cleaned_data.pkl"
    with open(file_path,'rb') as f:
        cleaned_data=pickle.load(f)
    print(f"Cleaned data loaded successfully")

    return cleaned_data

# Execute file loading
lda_model,all_original_text=load_lda_data()
mbti_cleaned_data=load_mbti_data()
		\end{python}
	
	To better interpret topics and detect patterns in the data, we construct a dedicated class LDATopicAnalyzer, which integrates the LDA model with the MBTI-annotated dataset. This class performs several key functions: it transforms input texts into bag-of-words representations, computes topic distributions for each document, and aggregates these distributions by MBTI type to observe group-level thematic tendencies. It also supports generating interactive visualizations through pyLDAvis, which is created and saved to html. Furthermore, it provides tools to extract representative keywords per topic and optionally identify and exclude noise topics that carry little interpretive value. 
	\begin{python}
# Create a class for LDA visualization
class LDATopicAnalyzer:
    def __init__(self,lda_model,texts,mbti_data):
        self.model=lda_model
        self.texts=texts
        self.mbti_data=mbti_data
        self.dictionary=lda_model.id2word
        self.corpus=[self.dictionary.doc2bow(text) for text in texts]
        self.noise_topic=[]
        self.topic_distributions=None
        self.mbti_topic_distributions=None
        
    def create_pyldavis_visualization(self,save_path=f"final_output/lda_visualization.html"):
        
        print("Creating pyLDAvis visualization...")
        # Prepare pyLDAvis visualization
        vis_data=gensimvis.prepare(
            self.model,
            self.corpus,
            self.dictionary,
            sort_topics=False
        )
        
        # Save as HTML file
        pyLDAvis.save_html(vis_data,save_path)
        print(f"pyLDAvis visualization saved to: {save_path}")
        
        # Display in notebook
        pyLDAvis.enable_notebook()
        return pyLDAvis.display(vis_data)
    
    def get_topic_distributions(self):
        """Get topic distributions for documents"""
        print("Calculating topic distributions...")
        
        topic_distributions=[]
        for doc_bow in tqdm(self.corpus,desc="Calculating topic distributions"):
            doc_topics=self.model.get_document_topics(doc_bow,minimum_probability=0)
            topic_probs=[prob for _,prob in doc_topics]
            topic_distributions.append(topic_probs)
        
        self.topic_distributions=np.array(topic_distributions)
        return self.topic_distributions
    
    def calculate_mbti_topic_distributions(self):
        """Calculate topic distributions for each MBTI type"""
        print("Calculating topic distributions for each MBTI type...")
        
        mbti_topic_dist={}
        
        for mbti_type in MBTI_types:
            if mbti_type in self.mbti_data and len(self.mbti_data[mbti_type].data) > 0:
                # Create corpus for documents of this MBTI type
                mbti_corpus=[self.dictionary.doc2bow(doc) for doc in self.mbti_data[mbti_type].data["posts"]]
                
                # Calculate topic distributions
                topic_sums=np.zeros(self.model.num_topics)
                doc_count=0
                
                for doc_bow in mbti_corpus:
                    doc_topics=self.model.get_document_topics(doc_bow,minimum_probability=0)
                    for topic_id,prob in doc_topics:
                        topic_sums[topic_id]+=prob
                    doc_count+=1
                
                # Calculate average topic distributions
                if doc_count>0:
                    mbti_topic_dist[mbti_type]=topic_sums/doc_count
                else:
                    mbti_topic_dist[mbti_type]=np.zeros(self.model.num_topics)
            else:
                mbti_topic_dist[mbti_type]=np.zeros(self.model.num_topics)
        
        self.mbti_topic_distributions=mbti_topic_dist
        return mbti_topic_dist
    
    def get_topic_words(self,num_words=10):
        """Get keywords for each topic"""
        topic_words={}

        for topic_id in range(self.model.num_topics):
            words=self.model.show_topic(topic_id,topn=num_words)
            topic_words[topic_id]=[word for word,_ in words]
        return topic_words
    
    def add_noise_topics(self,*topic_ids):
        """Define noise topics"""
        for i in topic_ids:
            self.noise_topic.append(i)
# Create analyzer instance

analyzer=LDATopicAnalyzer(lda_model,all_original_text,mbti_cleaned_data)
print("LDA topic analyzer created successfully!")

# Create pyLDAvis interactive visualization
vis=analyzer.create_pyldavis_visualization()
vis
	\end{python}

	The interactive topic visualization generated by pyLDAvis can be accessed at the following link: \url{https://dominicmin.github.io/Intro_to_DS_Assignment/lda_visualization.html}. Figure~\ref{fig:HTML} shows the initial state of the HTML file: 
	
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{Q2html0} 
			\caption{\centering Screenshot of the initial state of the HTML file}	
			\label{fig:HTML}	
		\end{figure}
	Note: The topic indices in the HTML file are offset by +1 compared to the LDA model output (i.e., Topic 1 in HTML = Topic 0 in LDA).
	
	As shown in Figure~\ref{fig:HTML},on the left side of the HTML file, the Intertopic Distance Map visualizes the relationships between topics in two dimensions. A possible interpretation is that each bubble represents a distinct topic, with its area indicating the proportion of tokens (words) in the entire corpus attributed to that topic. Larger bubbles correspond to topics that account for a greater share of the corpus. The position of each bubble reflects semantic similarity—the farther apart two bubbles are, the less semantically related the corresponding topics. However, as will be demonstrated later, this interpretation fails to accurately characterize certain topics.
	
	On the right side of the HTML page, the adjustable relevance metric of the selected topic is presented alongside the Top-30 Most Salient Terms. For each term, its overall frequency in the corpus is displayed as a blue histogram, while its estimated frequency within the selected topic is shown in red. Users can adjust the value of $\lambda$ to observe changes in term saliency and the relationship between a term’s overall frequency and its topic-specific relevance. 
	
	When $\lambda$ is slided to the position 0, the terms that are unique in this topic are prior exhibited. These words are characterized by high distinctiveness, meaning they appear much more frequently in the selected topic compared to their frequency across the entire corpus. As a result, they serve as strong discriminators, helping to differentiate this topic from others. Although such terms may not be the most frequent within the topic itself, they often carry greater semantic specificity and are particularly useful for interpreting nuanced or domain-specific themes. However, because this setting emphasizes uniqueness over prevalence, it may occasionally highlight low-frequency or noisy terms, which should be interpreted with caution in topic labeling.
	
	Meanwhile, when $\lambda$ is slided to the position 1, these words represent the most commonly occurring terms in the topic and therefore reflect its core semantic content. This setting is particularly useful for understanding the dominant themes or main discourse of the topic. However, because it does not account for how exclusive a term is to the topic, some high-frequency terms that are also common in other topics may be included, potentially reducing the distinctiveness of the topic’s representation.
	 
	From Figure~\ref{fig:HTML}, we can gain an overview of our modeling outcome. Topic 18 has the largest bubble, indicating that it is the most prevalent topic in the corpus. Topics 9, 15, 4, and 17 are located farther away from the other topics, suggesting a clear semantic distinction. In contrast, Topics 3, 13, 2, 8, 6, 19, and 18 exhibit notable spatial overlap, indicating only minor differences in their semantic content. In the whole corpus, the most reoccurring words are mostly related to the entertainment fandom and game fandom.
	%并不完全表示,没什么特点，联系岔开
	
	By selecting different topics and adjusting the value of $\lambda$, we can summarize the central theme of each topic, determine whether it is a noise topic, and assess its degree of semantic coherence.
	
	Take topic 12 as an example, when $\lambda$ is set to 0, the most relevant terms—such as trump, biden, republican, democrat, congress, and abortion—are highly distinctive and strongly associated with U.S. political discourse. These terms are not only topically specific but also exclusive to this topic, suggesting a focused and meaningful theme centered around American politics, government institutions, and social issues.	In contrast, when $\lambda$ is increased to 1, the top terms—such as state, country, child, gun, and law—shift toward higher-frequency words within the topic. Although some of these terms are more general, they still retain political relevance and semantic consistency with the topic’s core, indicating that the theme is robust across different relevance metrics. In conclusion, the presence of consistently interpretable and contextually appropriate terms at both extremes of the $\lambda$ scale demonstrates that topic 13 exhibits a relatively high degree of semantic coherence. The lack of function words, formatting artifacts, or off-topic vocabulary suggests that this is not a noise topic, but rather a well-defined and meaningful cluster within the corpus.
	
	By applying this method to all the topic, we can conclude the following topic modeling result:
	\begin{itemize}
	\item High semantic coherence - 8 topics:
	
		Topic 0: Astrology/Zodiac
		
		Topic 3: K-pop Groups (TXT/NCT)
		
		Topic 4: Western TV Shows (Stranger Things/Heartstopper)
		
		Topic 6: Gaming (Genshin Impact)
		
		Topic 8: K-pop Groups (SEVENTEEN/ATEEZ)
		
		Topic 9: Western Pop Music/Celebrities
		
		Topic 10: K-pop Group (BTS)
		
		Topic 14: K-pop Group (ENHYPEN)
		
		Topic 16: K-pop Group (Stray Kids)
	\item 	Medium semantic coherence - 6 topics:
	
		Topic 2: Academic/Business/Personal Development
		
		Topic 5: Gender Identity/LGBTQ+/Mental Health
		
		Topic 7: Food/Weight Management/Eating Disorders
		
		Topic 11: Merchandise Trading/Collectibles
		
		Topic 12: Politics/Social Issues
		
		Topic 17: Anime/Manga/Fan Culture
		
		Topic 1: Daily Life and traveling
	\item 	Low semantic coherence - 1 topic:
	
		Topic 15: K-pop Industry General
	\item Noise Topics - 2 topics:
	
		Topic 13: Mixed Social Media Expressions
		
		Topic 18: Generic Terms/Mixed Content
	\end{itemize}
	
	Interestingly, when linking the interpreted topics to their corresponding bubbles, we observe that although Topic 2 (Academic/Business/Personal Development) and Topic 12 (Politics/Social Issues) show considerable overlap in the Intertopic Distance Map, they are not semantically similar. This mismatch occurs because high-dimensional topic data is projected onto a two-dimensional space. 
	%需要写噪音去除
	
	
	\subsubsection{Topic Distribution}
	We begin by calculating and visualizing topic distribution.
	\begin{python}
# Calculate topic distributions for MBTI types

mbti_topic_dist=analyzer.calculate_mbti_topic_distributions()
topic_words=analyzer.get_topic_words()

# Create topic distribution DataFrame
topic_dist_df=pd.DataFrame(mbti_topic_dist).T
topic_dist_df.columns=[f"Topic {i}" for i in range(len(topic_dist_df.columns))]
topic_dist_df.drop([f"Topic {i}" for i in analyzer.noise_topic],
                   axis=1,
                   inplace=True)

print("MBTI type topic distribution calculation completed!")
print(f"Topic distribution matrix shape: {topic_dist_df.shape}")

# Display first few rows
print("\nPreview of topic distributions for each MBTI type:")
display(topic_dist_df.head())
	\end{python}
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{Q2row} 
		\caption{\centering Preview of first few rows}		
	\end{figure}
	
	A heatmap is generated based on the computed values to visually represent the results.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{Q2hmap} 	
	\end{figure}
	
	To assist with our interpretation with the heatmap, we calculate the overall popularity of each topic.
	\begin{python}
# Create topic keyword cloud summary
def create_topic_wordcloud_summary(topic_words,topic_dist_df):
    """Create topic keyword summary"""
    print("=" * 60)
    print("Topic keyword summary")
    print("=" * 60)
    
    # Calculate overall popularity of each topic
    topic_popularity=topic_dist_df.mean().sort_values(ascending=False)
    
    for i,(topic_idx,popularity) in enumerate(topic_popularity.items()):
        topic_num=int(topic_idx.replace('Topic',''))
        print(f"\nTopic {topic_num} (Popularity: {popularity:.3f}):")
        print(f"Keywords: {','.join(topic_words[topic_num][:8])}")
        
        # Find the MBTI type that most prefers this topic
        topic_col=f"Topic {topic_num}"
        if topic_col in topic_dist_df.columns:
            top_mbti=topic_dist_df[topic_col].nlargest(3)
            print(f"Most preferred MBTI types: {','.join([f'{mbti}({score:.3f})' for mbti,score in top_mbti.items()])}")
        
        if i >= 9:  # Only show top 10 topics
            break


create_topic_wordcloud_summary(topic_words,topic_dist_df)
	\end{python}
	 This gives us the following summary:
	 \begin{figure}[H]
	 		\centering
	 		\includegraphics[width=0.5\textwidth]{Q2topsum} 	
	 \end{figure}
	 
	 With the aid of the summary statistics, we interpret the heatmap and draw conclusions from two analytical perspectives.
	 
	 
	
	Analyzing from the perspective of individual topics, we can find distinct MBTI engagement patterns across different semantic domains. 
	\begin{itemize}
	\item Topic 15 (K-pop fandom activities): characterized by group debuts, comebacks, and concerts; demonstrates strong engagement among ISFP, ISFJ, and ESFP types, underscoring the intersection of sensory enjoyment and emotional engagement.
	\item Topic 14 (lightweight, emotionally-driven daily expression): resonates with SP types—particularly SFPs and STPs—who prioritize spontaneity and immediate experiential sharing.
	\item Topic 7 (sensory experiences related to food and comfort): emphasizes fast food, coffee, and cream, aligning with ISTP and ESTP preferences for direct sensory satisfaction.
	\item Topic 17 (strategic, abstract, and fandom-oriented discussions): involves anime, manga, and fan fiction; attracts ENTP, INFP, and ESTP types, indicating alignment with analytical and imaginative discourse.
	\item Topic 2 (experiential and intellectual reflections): appeals to INTJ, ENTJ, and INFJ types, reflecting their preference for profound, thoughtful, and self-reflective content.
	\item Topic 18 (adventurous and competitive themes): includes sports and competitions; engages ENTP, ESTP, and ENTJ personalities drawn to intense experiences.
	\item Topic 5 (identity and socially charged discussions): covers gender, sexuality, and societal roles; garners interest from ENTP, ESTP, and ESFP types open to explorative and provocative topics.
	\item Topic 12 (politically charged discourse): focuses on societal debates like Trump, guns, and abortion; attracts ENTJ, INTJ, and ESTJ types inclined toward policy and governance.
	\item Topic 10 (BTS fandom): features BTS member names; resonates with ESFJ and ISFJ types, highlighting community-oriented engagement.
	\item Topic 4 (Stranger Things franchise): references characters like Eddie and Steve; shows engagement from ISTP, ENTP, and INFP types, indicating analytical and introspective affinities.
	\end{itemize}
	
	Analyzing from the perspective of MBTI functional groups, clearer behavioral and cognitive preferences emerge.
	\begin{itemize}
	\item NF types consistently gravitate toward emotionally and symbolically rich discussions (Topics 1, 2), emphasizing their values-driven communication style and expressive tendencies. 
	\item SP types distinctly favor immediate experiential and sensory-focused topics (Topics 7, 14, 15), indicative of their present-oriented, sensory-driven inclinations. 
	\item NT types predominantly engage in abstract reasoning, strategic analyses, and fandom discussions that encourage complex intellectual engagement (Topics 2, 5, 17).
	\item SJ types display balanced interests, engaging prominently with mainstream cultural phenomena and structured, community-centric discussions (Topics 1, 10, 17), reflecting their preference for structured interaction, practical feasibility, and communal harmony.
	\end{itemize}
	
	\subsubsection{Topic Preference between MBTI Dimensions and Groups}
	

	
	
	
	
	
	\subsection{Expressive Patterns of MBTI Personalities}
	\subsubsection{Data Cleaning and Pre-processing}
	Expression pattern analysis shares the same initial steps as LDA modeling: removing URLs, emojis, and tags. Please refer to the LDA data-cleaning section for details. We only need to load the cleaned data for our expressive pattern analysis.
	\begin{python}
from data_clean import Data_to_Clean,Data_to_Analyze
import pickle
import os
import pandas as pd
import copy
import json
import logging
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
tqdm.pandas()

MBTI_types=[
    'istj', 'isfj', 'infj', 'intj', 
    'istp', 'isfp', 'infp', 'intp', 
    'estp', 'esfp', 'enfp', 'entp', 
    'estj', 'esfj', 'enfj', 'entj'
    ]
cleaned_data={T:None for T in MBTI_types}

for type in cleaned_data.keys():
    file_path=f"Data\\cleaned_data\\{type}_cleaned.pkl" 
    try:
        with open(file_path, 'rb') as f:
            cleaned_data[type] = pickle.load(f)
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
    except pickle.UnpicklingError:
        print(f"Error: Could not unpickle the file {file_path}. It might be corrupted or not a valid pickle file.")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
	\end{python}
	
	To check whether we have successfully imported the cleaned dataset, we print out the 0th column,  "posts" row of infp.
	\begin{python}
infp=cleaned_data["infp"]
infp.data.loc[0,"posts"]
	\end{python}
	
	From the output, we can see that there is no URLs, emojis, and tags in our data, which means that we have successfully cleaned and read from the cleaned dataset. Our data is ready for analysis.
	\begin{figure}[H]
		\centering
		\includegraphics{Q2VADER1} 
		\caption{Cleaned Data printed out}		
	\end{figure}
	
	\subsubsection{Linguistic Style Patterns of MBTI Types}
	We select 4 distinct linguistic dimensions to represent linguistic styles:
	\begin{enumerate}
		\item Sentence Quantity:A larger sentence quantity suggests that the user tends to express themselves in shorter, more frequent statements. This may reflect a conversational, spontaneous communication style, often associated with extraversion or emotional openness.  
		\item Word Count: Higher word count may indicate a more elaborative or expressive communication habit, suggesting the user tends to provide more context or detail. This could reflect traits such as thoughtfulness, emotional depth, or even analytical thinking.
		\item Upper-Case Ratio: A higher ratio of uppercase letters often signifies stronger emotional expression, emphasis, or a more dramatic tone. It may reveal higher emotional arousal or a more assertive communication style, which could be linked to personality traits like enthusiasm or dominance.
		\item Reading Ease(reflected by both Flesch and GF index):Higher reading ease scores imply simpler sentence structures and more familiar vocabulary, often seen in casual or informal communication. In contrast, lower scores suggest complexity and abstractness, which could indicate intellectual engagement, formality, or introversion. In our project, we adopted two kinds of rating systems to evaluate the reading ease of a context. The higher the Flesch Reading Ease score is, the easier the context can be understood; the lower the GF index is, the easier the context can be understood.
	\end{enumerate}
	
	These values are first computed before further analysis.
	\begin{enumerate}
		\item Class Definition \& Constructor
		\begin{python}
class Data_to_Analyze(Data_to_Clean):
    def __init__(self, type, source=raw_data):
        super().__init__(source)
        self.data = self.data.loc[self.data["type"] == type].reset_index(drop=True)
        self.data_to_vec = None
        self.basic_identities = pd.Series({
            "type": type,
            # Number of sentences in a post
            "sentence_quantity": [],
            "ave_sentence_quantity": None,
            # Number of words in a post
            "word_count": [],
            "ave_word_count": None,
            # Ratio of upper case characters in a post
            "upper_ratio": [],
            "ave_upper_ratio": None,
            # Two readability indicators: Flesch Reading Ease and Gunning Fog Index
            "reading_ease": [],
            "ave_reading_ease": None,
            "GF_index": [],
            "ave_GF_index": None,
            # Overall sentiment indicator (VADER)
            "overall_vader_score": None
        })

		\end{python}
		Defines a class Data\_to\_Analyze that inherits from Data\_to\_Clean.
		\begin{itemize}
			\item \textbf{Data subset} – The constructor calls super().\_\_init\_\_(source) to initialize the parent class, then filters the dataset so that only rows whose type column equals the target MBTI type remain (self.data).
			\item \textbf{Feature container} – basic\_identities is a pd.Series used to store a collection of text statistics: sentence count, word count, upper case ratio, readability metrics (Flesch Reading Ease and Gunning Fog Index) and an overall VADER sentiment score.
		\end{itemize}
		\item Sentence Count
		\begin{python}
def get_sentence_quantity(self):
    for post in self.data["posts"].values:
        self.basic_identities["sentence_quantity"].append(len(post))
    self.basic_identities["ave_sentence_quantity"] = ave(self.basic_identities["sentence_quantity"])
		\end{python}
		
		Iterates through every post (each post is already a list of sentences). The length of the list gives the number of sentences, which is appended to sentence\_quantity. Finally, the helper ave() computes their mean.
		\item Word Count
		\begin{python}
def get_word_count(self):
    for post in self.data["posts"].values:
        total = 0
        for sentence in post:
            total += len(sentence.split(" "))
        self.basic_identities["word_count"].append(total)
    self.basic_identities["ave_word_count"] = ave(self.basic_identities["word_count"])
		\end{python}
		
		For each post, it splits every sentence on whitespace to count words and sums them into total. The total per post is stored in word\_count, and the average is calculated afterwards.
		\item Upper Case Character Ratio
		\begin{python}
def get_upper_ratio(self):
    for post in self.data["posts"].values:
        char_count = 0
        upper_count = 0
        for sentence in post:
            for ch in sentence:
                if ch.isalpha():
                    char_count += 1
                    if ch.isupper():
                        upper_count += 1
        if char_count:
            self.basic_identities["upper_ratio"].append(upper_count / char_count)
    self.basic_identities["ave_upper_ratio"] = ave(self.basic_identities["upper_ratio"])
		\end{python}
		
		Traverses every character of every sentence, counting alphabetic characters (char\_count) and, among them, the upper case ones (upper\_count). The ratio per post is stored; the overall mean is then computed.
		\item Readability Metrics
		\begin{python}
def get_readability(self):
    reading_ease = []
    GF_idx = []
    for post in self.data["posts"].values:
        concatenated = post[0]
        for idx in range(1, len(post)):
            concatenated += post[idx]
        reading_ease.append(textstat.flesch_reading_ease(concatenated))
        GF_idx.append(textstat.gunning_fog(concatenated))
    self.basic_identities["reading_ease"] = reading_ease
    self.basic_identities["ave_reading_ease"] = ave(reading_ease)
    self.basic_identities["GF_index"] = GF_idx
    self.basic_identities["ave_GF_index"] = ave(GF_idx)
		\end{python}
		
		Each post’s sentences are concatenated into a single string, after which textstat computes Flesch Reading Ease and Gunning Fog Index. Both per post values and their averages are stored.
		% 文字格式不对
	\end{enumerate}
	
	The calculation results are printed and displayed.
	\begin{python}
mbti_identities={
    T.upper():{
        k:cleaned_data[T].basic_identities[k]
        for k in [
        "ave_sentence_quantity",
        "ave_word_count",
        "ave_upper_ratio",
        "ave_reading_ease",
        "ave_GF_index"
        ]
    }
    for T in MBTI_types
}
mbti_identities=pd.DataFrame(mbti_identities).T
mbti_identities
	\end{python}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{Q2wordchart} 
		\caption{Table of Linguistic Style Features of MBTI Social Media Posts}		
	\end{figure}
	
	In order to better interpret the underlying patterns in the table, we present descending histograms for visual analysis.
	\begin{python}
column_mapping={
    "ave_sentence_quantity":"Sentence Quantity",
    "ave_word_count":"Word Count",
    "ave_upper_ratio":"Upper Case Letter Ratio",
    "ave_reading_ease":"Reading Ease"
}
fig,axes=plt.subplots(2,2,figsize=(18,10))
axes=axes.flatten()
for i,col in enumerate(mbti_identities.columns[:4]):
    axes[i].bar(mbti_identities[col].sort_values(ascending=False).index, mbti_identities[col].sort_values(ascending=False))
    axes[i].set_ylabel(f"{column_mapping[col]}")
    axes[i].set_title(f"{column_mapping[col]}")
fig.suptitle("Linguistic Features of MBTI Social Media Posts",fontsize=14, fontweight='bold')
plt.legend()
plt.tight_layout()
plt.savefig("final_output/ling_features.png")
plt.show()
	\end{python}
\begin{figure}[H]
    \centering
    \adjustbox{max width=1.2\textwidth,center}{\includegraphics{Q2wordplot}}
\end{figure}
	The histograms allow the observation of surface-level patterns in linguistic styles.
	\begin{enumerate}
	\item Sentence Quantity: The top three MBTI types with the highest average sentence counts are ESTJ (150.1), ENTP (147.2), and ESFP (147.1). In contrast, the bottom three are INFP (135.3), INFJ (136.1), and ISFJ (139.9). This may indicate a correlation between the Extroversion/Introversion type and the extent of verbal expression. However, the overall variation in sentence quantity is relatively minor, suggesting limited explanatory significance.
	\item Word Count:
	\item Upper-Case Ratio:
	\item Reading Ease:
	\end{enumerate}
	
	 
	
	
	
	\subsubsection{Sentiment Analysis}
	Sentiment analysis, or opinion mining, is an active area of study in the field of natural language processing that analyzes people's opinions, sentiments, evaluations, attitudes, and emotions via the computational treatment of subjectivity in text. There are various approaches to sentimental analysis. In this assignment, we choose VADER(Valence Aware Dictionary for sEntiment Reasoning) as our tool to analyze the difference of different MBTI personalities in emotional expression due to its superiority in social media context(\cite{VADER}).
	
	VADER uses a pre-constructed system of "lexicon + weighted rules" to score each word in the input text and generate an overall sentiment score. For every given text, VADER first separates the text into words. Then, it rates the words from -4 to 4(4 as the most positive, and -4 as the most negative) according to its 7,500-entity valence lexicon. To increase the rating accuracy and distinguish between strength of emotions, VADER corrects the scores with a set of rules, which includes increasing or decreasing the score when identifying capitalization, negative and contrastive words. The details rating process is represented in the flow chart below.
	%插入图片
	

	\subsubsection{Exploratory Data Analysis (EDA)}
	
	

	\begin{python}
def get_sentence_quantity(self):
    for post in self.data["posts"].values:
        self.basic_identities["sentence_quantity"].append(len(post))
    self.basic_identities["ave_sentence_quantity"] = ave(self.basic_identities["sentence_quantity"])
	\end{python}
	
	


% ----------------- References ------------------------------
	\printbibliography[title={References}]

\end{document}
